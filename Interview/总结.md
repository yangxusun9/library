## 自我介绍

面试官好，我之前是在杭州的一家公司担任数据开发工程师，期间经历的项目有离线数仓和基于flink的实时分析平台，去年7月份来的深圳，也就是现在的公司任职至今。在这个公司数仓也做过，大数据组件平台开发也做过。可以说不管是业务上层还是架构底层都有所涉及。目前是因为公司架构调整，个人感觉对自己的职业规划有影响，所以打算换个平台继续发展。

## 项目介绍

### 平台

​	之前公司抽数用的是第三方的软件，但是存在支持数据源少，对数据库负载高的问题，为了解决这个问题，公司决定划分平台组来开发自研的抽数平台。但是我是有点自己的想法的，之前我有看过网易有数那套平台的相关文章，我就想其实我们要做的产品，跟这个是相似的。所以借鉴了目前市面上成熟的平台的一些经验，开发了元数据平台，抽数平台，任务监控平台，任务部署平台，统一计算引擎，支持配置多集群，多数据源，多张表,经过测试，吞吐量能达到每分钟千万级。接下来我简单讲下各个模块的原理：

- 抽数平台：利用debezium 作为一个插件，配合Kafka connect 来将业务库里的数据都发往Kafka中，之后通过Flink SQL 来写到对应的数据源（kudu，upsert_kafka等），值得一提的是，在往kudu里写的时候发现目前开源的connector使用起来有问题，不支持撤回流写入（识别不了主键），时态表等功能，我们自己改了源码，提供了这些功能。
- 元数据平台：在用户选择source表时，会从对应数据源拉取对应的元数据到我们平台的元数据库，然后根据用户选择的输出源进行自动的字段映射，同时也维护一份输出表元数据，而维护的这些元数据通过自动拼接DDL来在抽数平台和SQL开发平台上实现无DDL编程。
- 任务监控平台：基于zookeeper的监听器机制，每次任务启动时去注册一个临时节点，并启动监听器，一旦任务失败则会根据报警策略报警。
- 任务部署：目前只支持flink on yarn 部署，后续打算将spark，k8s集成进去。用户在前端传入相应的运行配置，抽数平台只需要点点点，sql开发平台需要输入transform SQL，然后把这些配置序列化存到redis配置中心中。其部署任务那块，主要参考flink-yarn 模块，先通过传入flink的配置（运行资源，引擎jar包），yarnClient,yarn conf ，yarn回调函数，生成集群描述器，然后调用部署方法即可完成部署
- 统一计算引擎：有点类似于zeppelin，分source，transform，sink，**目的是使用同一套代码，最大限度的去兼容不同的计算引擎，集群环境**。基于SQL开发环境，flink ，spark各包含一个main方法，主要工作就是读取配置中心，封装成方法入参，底层还是使用的env.excutesql（），不过为了避免一个表生成一个job，采用了StatmentSet,把任务都封装为了一个job，最大化节约资源。

### 云数仓

整体架构是 

- 数据采集： dataworks,http 请求，sdk,DTS
- 计算引擎： maxcompute,vvp
- 存储： oss，hologres，kafka，Maxcompute（盘古）
- 调度： Maxcompute（伏羲）
- 即系查询： Holoweb

因为计算用的都是阿里云的云上资源，能够更好的进行资源的利用。

采用的是lamda 架构，批处理和流处理分开计算，为了保证数据的最终一致性，每天会有一个同步任务将批处理的数据刷写到流处理的表中。

​	亮点：

1. 用户更改id时，需要将之前的数据也全部更改为最新的id，用到的是 自定义filter 算子，（所有事件都是来自于同一个topic），检测到 用户更改id 事件，则直接发 jdbc update 请求，

这个架构也有一个缺点，就是过于黑盒，比如登录不了服务器，查看不了进程gc状态，没有办法灵活的使用JVM参数进行一些调优。

### 离线数仓

之前公司数仓存在的问题主要有：

1. SQL冗余：每个宽表都是从ods直接取的，没有中间的建模过程，导致一个SQL有700多行是常有的事，很难维护。
2. 小文件：使用Data pipeline实时同步数据时，会产生大量小文件
3. 建表DDL与业务逻辑SQL混在一起，不易维护

所以参照领域驱动模型的一些思想，有以下几层结构：

1. driver：方法入口
2. Service: 具体的转化逻辑
3. repostity:通用方法，比如读取数据，写入数据
4. DDL：主要存放表的DDL，并且会维护一个JSON文件，存了表名和DDL文件的映射，到时候会根据表名去获取DDL语句来进行创建表的操作

同时我们也写了一个合并小文件的脚本，原理就是读出来写入到临时表，然后写回到表里,控制reduce个数为2；（支持动态传入）

总体建模有四层：source（主要针对实时同步的表，会含有DDL标识），ods，dwd，dim，DM

总体的数据流向：

- 业务库：polarDB，PG
- 抽数中间件：sqoop，dataWorks,datapipline
- 数仓：Hive
- 宽表交互：ADBPG

**离线数仓重构的效果有什么量化指标来衡量**？

## Flink 

### flink 使用中遇到什么问题

- flink 部署任务模块

- flink 改写connetor
- 实时数仓解决用户id变更，历史数据跟着变更的问题
- 抽mongo cdc join 任务OOM调优

### flink 调优

mongo CDC 抽取mongo表，70w的数据给了一个TM，5G内存，跑不动，看Web UI 有反压。

1. 首先把sink 端换成black hole，发现依然反压，排除掉sink端的问题

2. 设置参数，把算子链打断，观看各个算子任务的运行状况，发现问题出在了一个changelog normalize 的算子上

3. 这个算子的源码之前研究upsert-kafka 时有看过，它的作用其实可以简单理解为给数据去重，转为标准的changelog 流。实现原理是会把同一个key的上一行数据存入状态，然后当前行会去取这个状态，如果状态不存在，就是insert，存在的话，就是上一行为 upsert-before,当前行为 upsert-after。

4. 那么问题的答案就出来了，就是因为它底层回去存储每个key的状态，而我快照阶段所有key都是非重复的，所以就直接把内存占满了

   ```java
   static void processLastRowOnChangelog(
               RowData currentRow,
               boolean generateUpdateBefore,
               ValueState<RowData> state,
               Collector<RowData> out) throws Exception {
           RowData preRow = state.value();
           RowKind currentKind = currentRow.getRowKind();
           if (currentKind == RowKind.INSERT || currentKind == RowKind.UPDATE_AFTER) {
               if (preRow == null) {
                   // the first row, send INSERT message
                   currentRow.setRowKind(RowKind.INSERT);
                   out.collect(currentRow);
               } else {
                   if (generateUpdateBefore) {
                       preRow.setRowKind(RowKind.UPDATE_BEFORE);
                       out.collect(preRow);
                   }
                   currentRow.setRowKind(RowKind.UPDATE_AFTER);
                   out.collect(currentRow);
               }
               // normalize row kind
               currentRow.setRowKind(RowKind.INSERT);
               // save to state
               state.update(currentRow);
           } else {
               // DELETE or UPDATER_BEFORE
               if (preRow != null) {
                   // always set to DELETE because this row has been removed
                   // even the the input is UPDATE_BEFORE, there may no UPDATE_AFTER after it.
                   preRow.setRowKind(RowKind.DELETE);
                   // output the preRow instead of currentRow,
                   // because preRow always contains the full content.
                   // currentRow may only contain key parts (e.g. Kafka tombstone records).
                   out.collect(preRow);
                   // clear state as the row has been removed
                   state.clear();
               }
               // nothing to do if removing a non-existed row
           }
       }
   ```

   

5. 解决方案：

   - 根据flink的内存管理，这一部分主要属于 manage meomory ，所以需要调大这个manage momoery 的占比。快照阶段结束后再调小内存
   - 在社区里跟作者讨论了一下，如果能在数据进来的时候就判断是否是快照阶段，然后快照阶段跳过normalize，这样就能减少状态存储的压力了，但是还没有时间去实现

### Flink submit on yarn 的API改造

查看flink-yarn 源码，发现flink on yarn 的部署都是先createYarnClusterDescriptor，通过传入flink的配置（hadoop jars,flink jars,flink 总的运行配置（最大并行度），引擎jar包），yarnClient,yarnConf(加载Hadoop conf,

```
// 设置顶层hdfs实现为DistributedFileSystem
configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
```

),yarn回调函数（可查看集群信息），然后调用deployApplicationCluster（运行配置（内存，核心数），applicationName），即可完成部署。

### kudu connector 源码改造

当使用Kudu- connector往Kudu中写数时，发现不支持撤回流写入，跟源码发现他内部走的是老的API,输出用的是DataStreamSink，在SQL中指定的主键不生效，后来自己定义了Kudu的动态数据源DynamicTableSink，使用的输出是SinkFunctionProvider，并且在工厂生产之前传入了shchema信息，以便后续做key的校验（参考JDBCSINk）打通了kafka到kudu的撤回流写入功能。

### Flink窗口 tigger 

```java
 public TriggerResult onElement(
            Object element, long timestamp, TimeWindow window, TriggerContext ctx)
            throws Exception {
        if (window.maxTimestamp() <= ctx.getCurrentWatermark()) {
            // if the watermark is already past the window fire immediately
            return TriggerResult.FIRE;
        } else {
            ctx.registerEventTimeTimer(window.maxTimestamp());
            return TriggerResult.CONTINUE;
        }
    }
```



registerEventTimeTimer 并不是每来一条都回去注册一个定时器，而是内部维护了一个queue，每次会拿当前时间跟队列头部的时间进行比较，选最近的时间去注册

### 计算引擎jar包如何设计

​	基于Flink的设计思想，有Source，Transform，Sink阶段

- Source: 执行映射表的逻辑
- Transform: 利用StatamentSet,将多个执行SQL糅合为一个任务，节约资源
- Sink: 维护了一个catalog集合，根据输出数据源动态加载catalog去执行DDL操作

PS：想利用Spirngboot 创建对象是单例的特性，但是发现1.9版本的flink跟 SpringBoot整合有问题，俩个运行的不在同一个JVM上，会找不到对象，而且也不想引多余的jar包，所以自己实现了一下IOC来自动注入。

### 多个表抽数的时候，如果一个表出错，怎么恢复

只能重跑该任务下的所有表，但是表并不会很多，而且从维护了检查点，从检查点恢复也不会重跑很多数据。

- 任务量：一个任务最多只能包含一个database下10个小表，或者一个database下的5个大表。

### flink 检查点对齐原理

分布式快照，将检查点的保存和数据处理分离开：

1. jobManager 会向source 发送一个带有新checkpoint ID 的信息，以此行为开启检查点
2. source 接收到信息后 会先将状态存入状态后端，然后**将 数据保存地址 通过ack返回给jobmanager**，接着开始广播checkpoint barrier.
3. 下游算子会等待所有分区都接收到barrier，已经接收到barrier的分区会将接下来的数据缓存起来，而未接收到的barrier的分区则做正常的数据处理，当所有分区都接收到barrier的时候就会触发检查点的保存，通知jobmanager,然后将barrier往下游发送。
4. 当Sink 端也向job manager 确认完毕后，本次checkpoint 完成，**JM 收到所有成功的ack后，会向持久化存储中再备份一个 Checkpoint meta 文件**。

这里得提一下，当任务将其状态保存到检查点时，它其实处于一个阻塞状态，而状态又可能比较大，有可能会阻塞个几分钟，所以我们选用的是支持异步快照的RocksDB，它会创建一个本地的副本，然后开启另一个线程去复制到远程存储，减少了任务的等待时间。

### flink的非对齐检查点

https://blog.csdn.net/u013939918/article/details/107372805/?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.base&spm=1001.2101.3001.4242

![image-20210803104853270](/Users/sun9/IdeaProjects/library/picture/非对齐检查点.png)

1. 当算子的所有输入流中的第一个屏障到达算子的输入缓冲区时，立即将这个屏障发往下游（输出缓冲区）
2. 由于第一个屏障没有被阻塞，它的步调会比较快，超过一部分缓冲区中的数据。算子会标记两部分数据：一是屏障首先到达的那条流中被超过的数据（上面chanel中的9），二是其他流中位于当前检查点屏障之前的所有数据（下面channel中的9，7）。

**优点**：在 Barrier 进入输入 Channel 就马上开始快照。这可以从很大程度上加快 Barrier 流经整个 DAG 的速度，从而降低 Checkpoint 整体时长，解决在高反压情况下作业难以完成 Checkpoint 的问题；同时它以磁盘资源为代价，避免了 Checkpoint 可能带来的阻塞。

**缺点**：由于要持久化缓存数据，State Size 会有比较大的增长，磁盘负载会加重。
随着 State Size 增长，作业恢复时间可能增长，运维管理难度增加。

**与对齐检查点的差异**

1. 快照的触发是在接收到第一个 Barrier 时还是在接收到最后一个 Barrier 时。
2. 是否需要阻塞已经接收到 Barrier 的 Channel 的计算。

### flink的至少一次 检查点是如何工作的

有点类似于非对齐检查点，不会去缓存barrier先到的channel的数据，而是直接处理，**但也不会去对这部分数据（barrier之后的数据）进行标记**，等到barrier都到了之后开始状态的保存

https://blog.csdn.net/weixin_44904816/article/details/102675286



### flink checkpoint 什么情况会失败

先打一套检查点对齐原理----

在这中间的任何环节都可能会失败，但是在生产中最常碰见的就是数据倾斜导致的checkpoint超时

- 扯到生产上碰到的计算一个小时内每个地区的PV，最开始采用的方法是直接开窗再sum，然后发现检查点失败，某几个task还出现了反压，后来的解决思路：按照（地区+随机数）keyby之后开窗，然后用聚合算子aggregate，然后再keyby（去除随机数），聚合，算出最终值

### flink 是怎么从检查点恢复的

检查点文件里面其实相当于 一个KV存储，key 是 每个操作算子的 uid，恢复的时候根据uid去查找状态

### 状态特别大如何解决（计算一个小时PV）？窗口如何设计？

代码逻辑层面：

- 增量聚合与全量聚合搭配使用

运行配置层面：

- 状态大可以开启异步快照（默认都是开启的），
- 使用ROCKsDB的话还可以开启增量快照，
- 同时也可以增大检查点之间的间隔，避免浪费过多的资源在检查点保存上

### flink checkpoint 和savepoint 的区别

**checkpoint的侧重点是“容错”**，即Flink作业意外失败并重启之后，能够直接从早先打下的checkpoint恢复运行，且不影响作业逻辑的准确性。而**savepoint的侧重点是“维护”**，即Flink作业需要在人工干预下手动重启、升级、迁移或A/B测试时，先将状态整体写入可靠存储，维护完毕之后再从savepoint恢复现场。

savepoint是“通过checkpoint机制”创建的，所以savepoint本质上是特殊的checkpoint。

checkpoint面向Flink Runtime本身，由Flink的各个TaskManager定时触发快照并自动清理，一般不需要用户干预；savepoint面向用户，完全根据用户的需要触发与清理。

checkpoint的频率往往比较高（因为需要尽可能保证作业恢复的准确度），所以checkpoint的存储格式非常轻量级，但作为trade-off牺牲了一切可移植（portable）的东西，比如不保证改变并行度和升级的兼容性。savepoint则以二进制形式存储所有状态数据和元数据，执行起来比较慢而且“贵”，但是能够保证portability，如并行度改变或代码升级之后，仍然能正常恢复。

checkpoint是支持增量的（通过RocksDB状态后端），特别是对于超大状态的作业而言可以降低写入成本。savepoint并不会连续自动触发，所以savepoint没有必要支持增量。



作者：LittleMagic
链接：https://www.jianshu.com/p/6c389effc7ae
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

### Slot是一个对象还是一个线程

对象，通常指的就是TaskSlot 类。

### flink 与spark streaming 的区别

**处理模型**：Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批

（Micro-Batch）的模型。 重点！！！

**架构模型**：

Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、

Executor。

Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。

**时间机制**：

时间机制SparkStreaming支持的时间机制有限，只支持处理时间。 Flink

支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时

间。同时也支持 watermark 机制来处理滞后数据。

**容错机制**：

Spark的CheckPoint只能保证数据不丢失，但是无法保证不重复，Flink使

用两阶段提交来处理这个问题。

### flink 有shuffle吗

有，但是官方定义为partiiton，支持多种模式

- shuffle：随机分区
- rebalance：轮询
- rescale：也是轮询，当下游算子并行度是上游的整数倍时，会有极高的效率
- broadcast：广播，每个TaskManager上缓存一份数据，但是是存在内存中，数据量不能太大

### Flink 保证精准一次性

- Source:可重设数据读取位置

- Transform:检查点一致性算法

- Sink：

  - 事务性写入：

    - 俩阶段提交：

      1. 请求阶段：协调者向每个参与者发送事务请求，当参与者完成事务后会返回给协调者OK信号，如果都返回的是OK，则进入下一个阶段，否则进行回滚。
      2. 提交阶段：协调者向参与者发起事务提交通知，参与者提交事务，并释放资源

      存在问题：

      1. 单点故障：由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。
      2. 同步阻塞：执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

  - 幂等性写入

### 场景：size为一亿的手机号黑名单，拿这个黑名单来对通话记录流来做过滤，怎么做？布隆过滤器如何实现

利用位图，先把所有黑名单刷进位图里，然后每来一条通话记录，就去位图里去命中，命中了就过滤掉，没命中就保留

### flink 流如何进行数据的去重

- 布隆过滤器

- Code: 每个key维护一个布尔值当作状态，来一条去判断是否之前来过，来过就说明重复。

- SQL:  

  - 针对追加流，用 row_number 
  - 针对撤回流，用distinct

  其实底层调用的还是 `RowTimeDeduplicateFunction` ,这里面的实现逻辑就是 把上一行存进状态里，然后跟当前行判断是否重复，重复的话就把之前行和当前行附上操作类型往下发送，不重复就更新状态

  ```java
  if (generateUpdateBefore || generateInsert) {
              if (preRow == null) {
                  // the first row, send INSERT message
                  currentRow.setRowKind(RowKind.INSERT);
                  out.collect(currentRow);
              } else {
                  if (generateUpdateBefore) {
                      final RowKind preRowKind = preRow.getRowKind();
                      preRow.setRowKind(RowKind.UPDATE_BEFORE);
                      out.collect(preRow);
                      preRow.setRowKind(preRowKind);
                  }
                  currentRow.setRowKind(RowKind.UPDATE_AFTER);
                  out.collect(currentRow);
              }
          } else {
              currentRow.setRowKind(RowKind.UPDATE_AFTER);
              out.collect(currentRow);
          }
      }
  ```

  其实 upsert_kafka 也是一样的逻辑，在生成执行计划的时候会加入一个优化器`StreamExecChangelogNormalize` ，这里面就会去调用上面说的去重逻辑。

### 窗口不支持撤回流，怎么处理一小时之内的聚合业务

- 利用group by 时间字段 去聚合

- 也可以自定义json- format，把操作类型获取出来，当作append流处理，根据操作类型去自定义处理逻辑

### 窗口触发计算机制

trigger，

在 onELement 方法中去判断 当前时间是否大于 窗口关闭时间，如果是，就计算，不是就去调用注册定时器的方法。注册定时器的方法内部会维护一个时间队列，每次会拿传入时间跟队列头部时间进行比较，拿小的的时间去注册定时器，这样就节省了很多资源

### flink 定时器机制（收件箱）

### flink 某些 数据源 水位线不推进

**withIdleness**() 检测空闲数据源

### flink 异步 io 如何实现

主要用在跟数据库的连接上，比如hologres—connetor 就是用了同步和异步俩中方法去实现 lookup 功能的

### flink 背压机制

流控

### flink SQL转换为执行图 过程，如何拆分算子链

### Flink metric 底层实现

## Hive

### HIVE SQL 的执行过程

Hive和spark 的Sql解析器是使用的Antlr4，但是优化过程也借用了Calcite的基于代价的优化策略，flink用的是Calcite。

- 基于代价的优化策略，会单独起一个任务，代价衡量值是cpu和IO，计算各个Opertator Tree节点的代价总和，从而得到代价最小的执行计划

根据Hive 的架构图说，

![preview](/Users/sun9/IdeaProjects/library/picture/view.png)

![preview](/Users/sun9/IdeaProjects/library/picture/view1.png)

SQL解析过程：

1. Parser：将sql解析为AST（抽象语法树），会进行语法校验，AST本质还是字符串。
2. Analyzer：语法解析，生成QB（query block）
3. 逻辑执行计划解析，生成一堆Opertator Tree
4. 进行逻辑执行计划优化，生成一堆优化后的Opertator Tree
5. 物理执行计划解析，生成tasktree
6. 进行物理执行计划优化，生成优化后的tasktree，该任务即是集群上的执行的作业

**执行过程**

1. 用户提交查询等任务给Driver。
2. 编译器获得该用户的任务Plan。
3. 编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息。
4. 编译器Compiler得到元数据信息，对任务进行编译，先将HiveQL转换为抽象语法树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理的计划（MapReduce）, 最后选择最佳的策略。
5. 将最终的计划提交给Driver。
6. Driver将计划Plan转交给ExecutionEngine去执行，获取元数据信息，提交给JobTracker或者SourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作。
7. 获取执行的结果。
8. 取得并返回执行结果。
   ————————————————
   版权声明：本文为CSDN博主「wuyue_fighting」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
   原文链接：https://blog.csdn.net/qq_39093097/article/details/99690604

### Hive 调优

- 存储：列式存储，开启压缩

- 执行：行列过滤、map join、小文件优化（har归档，merger，JVM重用）、开启本地模式（小于128m或小于4个map则本地化运行），合理设置map，reduce个数

  

  重点说列式存储，开启了列式存储之后，就可以引用列式存储的查询杀器——**向量化查询**

  > 通常查询是每次只处理一行数据，每次处理都要走过较长的代码路径和元数据解释，从而导致CPU使用率非常低
  >
  > 而在向量化查询执行中，每次处理包含多行记录的一批数据，每一批数据中的每一列都会被存储为一个向量（一个原始数据类型的数组），这就极大地减少了执行过程中的方法调用、反序列化和不必要的if-else操作，大大减少CPU的使用时间。

  就像最近颇具争议的 StarRocks(),它为什么敢叫板市面上所有的olap 数据库，就是得益于它的全面向量化查询引擎和MPP架构

### Hadoop 触发 rebalance条件

1. 通过人工输入命令启动，会在当前节点启动一个进程 rebalance Server（为了避免给namenode带来过大的负担）
2. rebalance server会向NameNode请求一份数据节点报告，在收到报告之后，使用获得的信息，计算出网络拓扑、集群平均存储使用率，然后把各个数据节点分成过载节点、负载节点、存储使用率高于平均水平的节点和低于平均水平的节点四类，再判断是否有节点处于过载和负载状态（也即过载节点列表和负载节点列表中是否有机器），如果是则继续，否则退出。如果判断可继续，则遍历过载节点列表和负载节点列表以生成Rebalance策略
3. Rebalance Server 向Name Node请求每个source节点的部分块分布报告（partial block report），请求的形式类似，默认size是1GB。所谓部分块报告，是指每次要求和返回的的只是加起来能满足size大小的block的信息，而非全部的block信息。
4. namenode随机挑选一些block，使得block的大小加起来等于请求中size的大小（见上一步，默认1GB），然后将被选中的block信息返回给rebalance server。
5. rebalance server 在返回的这些block信息中挑选出每个source上需要移动的block，直到选出的block的大小达到了前面提到过的阈值（见本节2.b中“如果source节点是过载节点……”一段）或者所有的block都被检查过了一遍，然后发往移动任务队列
6. 所有的block被扫描了一遍后，重复步骤3
7. 所有的移动计划已经完成，并且队列中没有任务之后，重复步骤2

### yarn 集群资源有限，然后提交了大任务上去，如何保证任务运行的高效

CPU 超卖可以解决 任务提交不上的问题

容量调度队列中配置优先级

### 小文件如何解决

- Har 归档

- Sequence file
- CombineFileInputFormat
- 工作中解决是把数据读出来，并行度调为1 ，再覆盖写进去

## Hbase

### Hbase rowKey设计

原则：长度（不要过长，会影响检索效率），散列，唯一

主键+hash

### Hbase 的row key 过长该如何优化

可以考虑使用md5 来对数据进行一定程度的压缩

### 二级索引原理

把索引当做主键，row key 当做 value ，维护在一张表里，先通过索引找到row key，然后再去命中 row key

### Hbase 是怎么 提高存储效率

把所有更新操作当做插入来处理

**优化点：**

- 开启压缩或者编码

### HBase block encoding，选择编码还是压缩

**压缩：**Hbase支持的压缩有gzip，snappy，lzo

**编码：**Prefix | Diff | Fast_Diff | Prefix_Tree

> 下面能不说就不说

在读写性能上的影响：

- 压缩：因为压缩是在flush 阶段发生的，所以对写性能没有多大影响。对读阶段而言，如果数据在内存中，则不会有解压，如果是在磁盘中，则会有解压，会造成影响。
- 编码：编码也是在flush 阶段发生的，所以对写性能没有多大影响。数据块是以编码形式缓存到blockcache中的，因此同样大小的blockcache可以缓存更多的数据块，这有利于读性能。另一方面，用户从缓存中加载出来数据块之后并不能直接获取KV，而需要先解码，这却不利于读性能。所以很玄学，得实际测。

根据网上的性能压测，发现只启用Prefix_Tree 性能和cpu 负载 最佳，但是具体生产环境还得再做测试。

### Hbase为什么采用列族存储

因为Hbase底层存储依托于HDFS，如果行式存储不适用于做olap查询，而列式的话，因为一列一个文件，可能会有小文件的问题，所以采用了列族存储（个人观点）

### HBASE REGION 的Rebalance 触发条件

- 触发条件：
  - 自动触发：如果开启balance_switch 参数，在HMaster中，后台会起一个线程定期检查是否需要进行rebalance，线程叫做BalancerChore。线程每隔 hbase.balancer.period会定期执行 master.balance()函数，配置项默认300000毫秒，5分钟。每次balance最多执行hbase.balancer.max.balancing，如果没有配置，则使用hbase.balancer.period配置项的值。
  - 手动触发：balancer
- 负载均衡原理：

1. 计算均衡值的区间范围，通过总Region个数以及RegionServer节点个数，算出平均Region个数，然后在此基础上计算最小值和最大值

   ```
   # hbase.regions.slop 权重值，默认为0.2
   最小值 = Math.floor(平均值 * (1-0.2))
   最大值 = Math.ceil(平均值 * (1+0.2))
   ```

2. 遍历超过Region最大值的RegionServer节点，将该节点上的Region值迁移出去，直到该节点的Region个数小于等于最大值的Region

3. 遍历低于Region最小值的RegionServer节点，分配集群中的Region到这些RegionServer上，直到大于等于最小值的Region

4. 负责上述操作，直到集群中所有的RegionServer上的Region个数在最小值与最大值之间，集群才算到达负载均衡，之后，即使再次手动执行均衡命令，HBase底层逻辑判断会执行忽略操作

## Kafka

### Kafka如何保证数据的稳定性

- ISR机制
- ack应答机制
- 故障处理：HW、LEO

### Kafka 消息回溯原理

其底层原理是通过timeindex文件找到对应的相对偏移量，从而得到offset，然后通过调用KafkaConsumer.seek方法，重设其offset

做法

 ``` bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group groupname --reset-offsets --all-topics --to-datetime 2019-09-15T00:00:00.000``` 

### Kafka的二阶段提交

Kafka 的二阶段提交是跟检查点一起发挥作用的，因为TwoPhaseCommitSinkFunction也继承了CheckpointedFunction接口。

- 请求阶段：

  每当需要做checkpoint时，JobManager就在数据流中打入一个屏障（barrier），作为检查点的界限。屏障随着算子链向下游传递，每到达一个算子都会触发将状态快照写入状态后端(state BackEnd)的动作。当屏障到达Kafka sink后，触发preCommit(实际上是KafkaProducer.flush())方法刷写消息数据，但还未真正提交。接下来还是需要通过检查点来触发提交阶段。

- 提交阶段：

  通过TwoPhaseCommitSinkFunction.notifyCheckpointComplete()方法。当所有检查点都成功完成之后，会调用KafkaProducer.commitTransaction()方法，正式向Kafka提交事务。

**数据可见性**：当使用事务向 Kafka 写入数据时，请将所有从 Kafka 中消费记录的应用中的 `isolation.level` 配置项设置成实际所需的值（`read_committed` 或 `read_uncommitted`，后者为默认值）

俩阶段提交：

1. 请求阶段：协调者向每个参与者发送事务请求，当参与者完成事务后会返回给协调者OK信号，如果都返回的是OK，则进入下一个阶段，否则进行回滚。
2. 提交阶段：协调者向参与者发起事务提交通知，参与者提交事务，并释放资源

存在问题：

1. 单点故障：由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。
2. 同步阻塞：执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

### Kafka partition 消费堵塞原因分析（感觉像问的是消费能力不足）

1）如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）

2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间<生产速度），使处理的数据小于生产的数据，也会造成数据积压。

### Kafka partition 和消费者数量关系

partition 数量 >= consumer 数量

再讲一下消费策略：

range - 首先会统计一个消费者，一共订阅了哪些主题！以主题为单位，  根据     主题的分区数 / 当前主题订阅的消费者个数 ，根据结果，进行范围的分配！

Robin - 以主题为单位，将主题的分区进行排序，排序后采取轮询的策略，将主题轮流分配到订阅这个主题的消费者上！

sticky- 也是轮询的方式，不过rebalance 时会保证还存在的消费者已分配的分区不动。

## Spark

### spark 调优

所有调优无非就是针对**CPU，IO，内存**这三点。

- 集群资源的优化：增加 driver，excutor 的内存，核心数
- 算子的优化：reduceby  、 filter 之后 coalesce 、
- RDD的优化：广播变量、缓存、Kryo序列化、调节本地化等待时间（3s）
- Shuffle的优化：reduce缓冲区大小、reduce 拉取间隔时间、拉取失败重试次数、调节sort shuffle阈值
- JVM调优：堆外内存、连接等待时间（防止因为gc时间过长而报错）

### spark driver 和 executor  之间是如何协作的

![image-20210930101701771](/Users/sun9/IdeaProjects/library/picture/image-20210930101701771.png)

### spark 如何 区分 代码是在 driver 执行还是 executor 执行

所有对RDD具体数据的操作都是在executor上执行的，所有对rdd自身的操作或者其他无关的操作，比如 system.out.print 都是在driver上执行的

### 数据去重方式，以及优缺点

- distinct ：对select 后面所有字段进行去重。会把所有数据发到一个reduce里面，效率较低
- Group  by: 对group by 后的字段进行去重，会有多少组发送给多少个reduce，效率高
- Row_number:语法上比group by更加灵活，但是有个排序的过程，效率稍低。

### map ,map partition 的区别

- map是对rdd中的每一个元素进行操作；一个function要执行的次数等于元素的个数
- mapPartitions则是对rdd中的每个分区的迭代器进行操作，把一个分区的数据都收集起来进行消费，一个分区只执行一次function，但是数据量大的时候就会OOM。

### Spark 和MR的区别

1. **基于内存数据处理模式**

   spark是基于内存进行数据处理的，MapReduce是基于磁盘进行数据处理

   的

2. **DAG计算模型**

   Spark 计算比 MapReduce 快的根本原因在于 DAG 计算模型。一般而言，

   DAG 相比MapReduce 在大多数情况下可以减少 shufflfflffle 次数。Spark 的 

   DAGScheduler 相当于一个改进版的 MapReduce，如果计算不涉及与其

   他节点进行数据交换，Spark 可以在内存中一次性完成这些操作，也就是

   中间结果无须落盘，减少了磁盘 IO 的操作。但是，如果计算过程中涉及数

   据交换，Spark 也是会把 shufflfflffle 的数据写磁盘的！

3. **资源申请粒度**

   spark是粗粒度资源申请，也就是当提交spark application的时候，

   application会将所有的资源申请完毕，如果申请不到资源就等待，如果申

   请到资源才执行application，task在执行的时候就不需要自己去申请资

   源，task执行快，当最后一个task执行完之后task才会被释放。

   **优点是执行速度快，缺点是不能使集群得到充分的利用**

   MapReduce是细粒度资源申请，当提交application的时候，Task执行时，

   自己申请资源，自己释放资源，task执行完毕之后，资源立即会被释放，

   task执行的慢，application执行的相对比较慢。

   优点是集群资源得到充分利用，缺点是application执行的相对比较慢。

   **Spark是基于内存的，而MapReduce是基于磁盘的迭代**

### spark OOM 怎么分析以及解决方法

​	首先需要了解Spark 的内存管理机制，他主要分为三块，缓存内存，计算内存，其他内存（存数据结构，元数据等信息），缓存内存和计算内存之间是动态管理的。

​	接下来在说一下OOM发生的地点，Driver端和Excutor 端。

Driver ： 1. 定义了一个很大的变量（缓存）

​				2.把所有数据拉到driver 处理（缓存）

​				3.spark UI 也会占据一部分内存（计算）

解决方案：加内存

Excutor:

- 1.广播了一个大变量到blockmanager（缓存）

- 2.数据倾斜（计算）

- 3.reduce缓冲区过大（计算）

解决方案：加内存-》针对数据倾斜：1.提高shuffle并行度2.先过滤

3.加盐4.join 类（mapjoin ， 稀释扩容）

## KUDU

### 为什么选用Kudu，Kudu 为什么比Hbase 快

其实主要还是根据我们公司的业务场景来定的，我们对写入性能并没有太高要求，反而对查询性能要求很高，而对比这俩个框架，显然kudu 的查询性能比 Hbase 高。

**原因：**

- Hbase 因为支持用户传入 timestamp 来定义数据的版本，所以必须要结合多个storefile进行查询，而kudu 因为不支持用户传入timestamp，所以它的timestamp 是递增的，所以只需要倒序去检索DeltaFile文件就能拿到最新的数据。
- kudu 因为维护了主键的索引，而且在 DiskRowSet 中维护了一个区间树，每个节点中维护有多个RowSet的最小键和最大键，所以在O(logn)时间内就能找到对应的数据
- 纯列式存储可以支持向量化查询。

读写过程 https://blog.csdn.net/wangyiyungw/article/details/82701414

存储结构 https://www.jianshu.com/p/5ffd8730aad8

### kudu 和Hbase的异同

- 不同：

1. Hbase依托于zookeeper，利用其来存储元数据表位置，master位置，RegionServer的工作状态，而Kudu将这些工作都交给了Master
2. Hbase将数据持久化这部分的功能交给了Hadoop中的HDFS，最终组织的数据存储在HDFS上。依靠 HDFS保证数据可靠性。Kudu自己将存储模块集成在自己的结构中，内部的数据存储模块通过Raft协议来保证leader Tablet和replica Tablet内数据的强一致性，和数据的高可靠性。为什么不像HBase一样，利用HDFS来实现数据存储，猜测可能是因为HDFS读小文件时的时延太大，所以Kudu自己重新完成了底层的数据存储模块，并将其集成在TServer中。
3. hbase列族存储，kudu列式存储
4. 针对相同主键更新操作的数据，Hbase是允许多版本的数据存在的，所以直接当做一条新插入的数据，而kudu只允许有一条存在，所以会将更新操作单独存放于内存，等待刷新或读取的时候再合并。

- 相同：

1. 都是通过数据的timestamp字段来实现mvcc，但是**hbase可以手动指定，kudu不能**
2. 都是LSM树结构

### CAP模型是那种

CP模型

### Kudu 的调优

最开始使用的时候发现Kafka 到 kudu 数据写入慢，排查出来是kudu sink 的问题

从俩个方向去解决

**硬件：**从机械硬盘改为固态

**软件配置：**

>  从 kudu 数据写入机制讲

```
1,maintenance_manager_num_threads
        Kudu后台对数据进行维护操作，如flush、compaction、inserts、updates、and deletes，一般设置为4，官网建议的是数据目录的3倍，
2,block_cache_capacity_mb
        分配给Kudu Tablet Server 块缓存的最大内存量，建议是2-4G
3,memory_limit_hard_bytes
        写性能，Tablet Server 能使用的最大内存量，建议是机器总内存的百分之80，master的内存量建议是2G，Tablet Server在批量写入数据时并非实时写入磁盘，而是先Cache在内存中，在flush到磁盘。这个值设置过小时，会造成Kudu数据写入性能显著下降。对于写入性能要求比较高的集群，建议设置更大的值
4，--flush_threshold_mb --memory_pressure_percentage
将数据从内存刷到磁盘减轻内存的压力，调整后台刷数据的任务包括FlushMRSOp和FlushDeltaMemStoresOp的启动阈值以及tablet server的背压阈值 
```

## Clickhouse

### 为什么快

- C++ 编写，能对查询进行加速（kudu也是）
- 列式存储，向量化执行引擎
- 稀疏索引（类似于 kudu 的主键区间树）
- 存储执行耦合，避免网络开销
- 数据存储在 ssd

## MySql

### Mysql buffer pool

**概念：**Buffer Pool就是数据库的一个内存组件，里面缓存了磁盘上的真实数据，然后我们的系统对数据库执行的增删改操作，其实主要就是对这个内存数据结构中的缓存数据执行的。

为了防止因为宕机导致内存中的更改丢失，所以mysql 有 redo log 机制。

**其中包含的三大链表**：

   ```
   # free链表
       用来存放空闲的缓存页的描述数据，如果某个缓存页被使用了，那么该缓存页对应的描述数据就会被从free链表中移除
    
   # flush链表
       被修改的脏数据都记录在 Flush 中，同时会有一个后台线程会不定时的将 Flush 中记录的描述数据对应的缓存页刷新到磁盘中，如果某个缓存页被刷新到磁盘中了，那么该缓存页对应的描述数据会从 Flush 中移除，同时也会从LRU链表中移除（因为该数据已经不在 Buffer Pool 中了，已经被刷入到磁盘，所以就也没必要记录在 LRU 链表中了），同时还会将该缓存页的描述数据添加到free链表中，因为该缓存页变得空闲了。
    
   # LRU链表（冷热分离）
       数据页被加载到 Buffer Pool 中的对应的缓存页后，同时会将缓存页对应的描述数据放到 LRU 链表的冷数据的头部，当在一定时间过后，冷数据区的数据被再次访问了，就会将其转移到热数据区链表的头部，如果被访问的数据就在热数据区，那么如果是在前25%就不会移动，如果在后75%仍然会将其转移到热数据区链表的头部
   ```

**并发支持**

 **Buffer Pool 一次只能允许一个线程来操作，一次只有一个线程来执行这一系列的操作，因为MySQL 为了保证数据的一致性，操作的时候必须缓存池加锁，一次只能有一个线程获取到锁**。

但是可以有多个Buffer Pool（通过参数配置）；

**数据页会不会加载到不同的Buffer Pool中**

MySQL 在加载数据所在的数据页的时候根据这一系列的映射关系判断数据页是否被加载，被加载到了那个缓存页中，所以 MySQL 能够精确的确定某个数据页是否被加载，被加载的到了哪个缓存页，绝不可能出现重复加载的情况

https://blog.csdn.net/shuaisunny/article/details/119144107

### Mysql 事务机制

靠redo log 和 undo log 来实现，redo 记录数据的更改，undo 记录原始数据，方便事务回滚以及实现mvcc。

**区别：**

redo：是先记录操作日志，虽然记录操作日志也是应用了缓存（innodb_log_buffer）但是它还是比数据更新之前更新redo日志到磁盘日志文件中，数据的更新会在后面的线程刷新操作过程中被更新，redo操作事务提交后只有日志被持久化数据暂时未被持久化。

undo:undo操作也使用了缓存，只是它在事务提交的时候会同时将数据和日志更新到磁盘，这步操作就是和redo的主要区别，并且该操作对磁盘IO的消耗非常大，所以undo操作保证了事务的原子性，事务一旦提交数据也被持久化了。

### Mysql 的MVCC

InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现。这两个列一个保存了行的创建时间，一个保存行的过期时间（删除时间）。当然存储的并不是真实的时间而是系统版本号（system version number）。每开始一个新的事务，系统版本号都会自动新增。事务开始时刻的系统版本号会作为事务的版本号，用来查询到每行记录的版本号进行比较。

### MySql 事务隔离级别

要解决的问题：

- **脏读**

脏读指的是读到了其他事务未提交的数据，未提交意味着这些数据可能会回滚，也就是可能最终不会存到数据库中，也就是不存在的数据。读到了并一定最终存在的数据，这就是脏读。

- **不可重复读**

对比可重复读，不可重复读指的是在同一事务内，不同的时刻读到的同一批数据可能是不一样的，可能会受到其他事务的影响，比如其他事务改了这批数据并提交了。通常针对数据**更新（UPDATE）**操作。

- **幻读**

幻读是针对数据**插入（INSERT）**操作来说的。假设事务A对某些行的内容作了更改，但是还未提交，此时事务B插入了与事务A更改前的记录相同的记录行，并且在事务A提交之前先提交了，而这时，在事务A中查询，会发现好像刚刚的更改对于某些数据未起作用，但其实是事务B刚插入进来的，让用户感觉很魔幻，感觉出现了幻觉，这就叫幻读。

概念：

- 读未提交：不加锁
- 读提交：一个事务只能读到其他事务已经提交过的数据，也就是其他事务调用 commit 命令之后的数据
- 可重复读：事务不会读到其他事务对已有数据的修改，即使其他事务已提交，也就是说，事务开始时读到的已有数据是什么，在事务提交前的任意时刻，这些数据的值都是一样的。但是，对于其他事务新插入的数据是可以读到的，这也就引发了幻读问题，**mysql 通过行锁和间隙锁的组合 Next-Key 锁解决了幻读问题**
  - **间隙锁** 其实相较于行锁，锁定的是一个范围
- 串行化：将事务的执行变为顺序执行，与其他三个隔离级别相比，它就相当于单线程，后一个事务的执行必须等待前一个事务结束

对比：

| 隔离级别 | 脏读   | 不可重复读 | 幻读   | 优缺点                                                       |
| -------- | ------ | ---------- | ------ | ------------------------------------------------------------ |
| 读未提交 | 可能   | 可能       | 可能   | 不加锁，效率最高，同时也最不安全                             |
| 读提交   | 不可能 | 可能       | 可能   |                                                              |
| 可重复读 | 不可能 | 不可能     | 可能   | 默认事务级别，mysql 通过行锁和间隙锁的组合 Next-Key 锁解决了幻读问题 |
| 串行化   | 不可能 | 不可能     | 不可能 | 最安全，将事务的执行变为顺序执行，相当于单线程，但是效率最低 |

https://zhuanlan.zhihu.com/p/117476959

### MySQL 为什么不支持大数据量

因为单机部署，大表的索引即使是B+树， 他的树高也会很高，磁盘io次数也就变高。

但是分布式就不同，每个机器管理不同的数据块，维护各自的索引，索引大小不会很大

### 介绍 Mysql 索引

B+ 树 比B树好的原因

- b+树因为把数据都存放在叶子节点里，所以非页节点存放的数据比b树多得多，树高也就比b树少很多，查询效率也就高的多
- b+树存放数据的叶子节点间有联系，可以做范围查询

### 索引的创建原则

-  为常作为查询条件的字段建立索引
-  对于查询频率高的字段创建索引
-  索引的数目不宜太多
-  选择唯一性索引
-  表小于2000 不要建索引

### 索引的优化原则

- 最左前缀匹配原则，联合索引，mysql会从做向右匹配直到遇到范围查询(>、<、between、like)就停止匹配，比如a = 1 and b = 2 and c > 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整
- 索引列不能参与计算
- join 语法，尽量将小的表放在前面，在需要on的字段上，数据类型保持一致，并设置对应的索引，否则MySQL无法使用索引来join查询

### 为什么是最左前缀匹配原则

联合索引(A,B,C)是一棵B+Tree，其非叶子节点存储的是第一个关键字的索引，而叶节点存储的则是三个关键字A、B、C三个关键字的数据，且按照A、B、C的顺序进行排序。

### Innodb 存储非主键索引（回表）

非主键索引 的叶子节点存的是主键值，查询的时候先查到主键值，在根据主键值去查相应的数据

**目的：** 节约空间，一行数据只会存在一个叶子节点里面

### 索引覆盖

索引覆盖是一种避免回表查询的优化策略。具体的做法是将要查询的数据作为索引建立普通索引（可以是单列索引，也可以是一个索引语句定义所有要查询的列，即联合索引），这样的话就可以直接返回索引中的数据，不需要再通过集聚索引去定位行记录，避免了回表的情况发生。

### mybatis 的防SQL注入

MyBatis启用了预编译功能，在SQL执行前，会先将上面的SQL发送给数据库进行编译。

具体使用(主要是# 的使用)

```xml
  <select id="selectByPrimaryKey" resultMap="BaseResultMap"         
                        parameterType="java.lang.Integer" >
    select 
    <include refid="Base_Column_List" />
    from user
    where id = #{id,jdbcType=INTEGER}
  </select>
```

**\#** 将sql进行预编译"where id = ?"，然后底层再使用PreparedStatement的set方法进行参数设置。

$ 将传入的数据直接将参数拼接在sql中，比如 “where id = 123”。

MyBatis是如何做到SQL预编译的呢？其实在框架底层，是JDBC中的PreparedStatement类在起作用，PreparedStatement是我们很熟悉的Statement的子类，它的对象包含了编译好的SQL语句。

## 数仓理论

### 星型模型和雪花模型的区别

雪花模型因为没有数据冗余需要关联，而星型不需要关联但是有数据冗余，星型效率高；

### 数据倾斜解决思路

很多数据倾斜的问题，都可以用和平台无关的方式解决，比如更好的***\*数据预处理\****，***\*异常值的过滤\****等。因此，解决数据倾斜的重点在于对数据设计和业务的理解，这两个搞清楚了，数据倾斜就解决了大部分了。

***\*1\*******\*）业务逻辑\****

我们从业务逻辑的层面上来优化数据倾斜，比如上面的两个城市做推广活动导致那两个城市数据量激增的例子，我们可以单独对这两个城市来做count，单独做时可用两次MR，第一次打散计算，第二次再最终聚合计算。完成后和其它城市做整合。

***\*2\*******\*）程序层面\****

比如说在Hive中，经常遇到count(distinct)操作，这样会导致最终只有一个Reduce任务。

我们可以先group by，再在外面包一层count，就可以了。比如计算按用户名去重后的总用户量：

（1）优化前 只有一个reduce，先去重再count负担比较大：

select name,count(distinct name)from user;

（2）优化后

// 设置该任务的每个job的reducer个数为3个。Hive默认-1，自动推断。

set mapred.reduce.tasks=3;

// 启动两个job，一个负责子查询(可以有多个reduce)，另一个负责count(1)：

select count(1) from (select name from user group by name) tmp;

***\*3\*******\*）调参方面\****

Hadoop和Spark都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。

比如 spark.sql.shuffle.partitions 提高reduce端的并行度

​		join中有小表的情况 广播小表，mapjoin

***\*4\*******\*）从业务和数据上解决数据倾斜\****

很多数据倾斜都是在数据的使用上造成的。我们举几个场景，并分别给出它们的解决方案。

l 有损的方法：找到异常数据，比如ip为0的数据，过滤掉

l 无损的方法：对分布不均匀的数据，单独计算

l 先对key做一层hash，先将数据随机打散让它的并行度变大，再汇集

l 数据预处理

### OLAP 一般用哪些技术

- MPP架构（大规模并行引擎）：Spark SQL ，Presto，Impala-kudu，
- 预处理架构：Druid，Kylin
- 搜索引擎架构： ES
- 列式数据库： ClickHouse

### lamada 架构如何理解

**lamada：**

- 优点：保存全部历史数据，方便重计算，灵活
- 缺点：难维护

**kappa：**

- 优点：一套代码，批流统一，方便维护
- 缺点：不够灵活

### 行式存储和列式存储

**行式存储**：

1、适合随机的增删改查操作;

2、需要在行中选取所有属性的查询操作;

3、需要频繁插入或更新的操作，其操作与索引和行的大小更为相关。

**列式存储**

1、查询过程中，可针对各列的运算并发执行(SMP)，最后在内存中聚合完整记录集，最大可能降低查询响应时间;（向量化查询）

2、可在数据列中高效查找数据，无需维护索引(任何列都能作为索引)，查询过程中能够尽量减少无关IO，避免全表扫描;

3、因为各列独立存储，且数据类型已知，可以针对该列的数据类型、数据量大小等因素动态选择压缩算法，以提高物理存储利用率；如果某一行的某一列没有数据，那在列存储时，就可以不存储该列的值，这将比行式存储更节省空间。

4、拿parquet 来举例，得益于parquet 的File metadata，记录了每一个Row group的Column statistic，包括数值列的max/min，字符串列的枚举值信息，可以通过这些信息过滤掉一些不需要访问的row group。**可以说对谓词下推很友好**。

### SQL 谓词下推机制，parquet 文件格式 谓词下推

简单来说就是在生成逻辑执行计划的时候，会去把过滤条件放到靠近数据源的位置，减少扫描数据量

Parquet中File metadata记录了每一个Row group的Column statistic，包括数值列的max/min，字符串列的枚举值信息，可以通过这些信息过滤掉一些不需要访问的row group。

### 拉链表解决了什么问题，如何加快查询效率

大表，存在update操作，会查看历史状态

**加速：**

1. 在一些查询引擎中，我们对start_date和end_date做索引，这样能提高不少性能。
2. 保留部分历史数据，比如说我们一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。

### 拉链表实现步骤

![image-20210412235413965](/Users/sun9/Library/Application Support/typora-user-images/image-20210412235413965.png)

### 增量数据与全量数据合并，如何实现

- 增量与全量 union 在一起，然后根据id去重

- 比如今天的分区是20210412，更改的数据包含了20210412，20210410的分区，那么

  (旧数据里20210412和20210410的分区的所有数据) Full join （今天的数据），然后用if根据是否为null做处理。

### 数据质量监控

现在配置的规则 就是 空值检测，表行数七天波动率，

然后还有数据运营根据业务规则圈定一批特殊用户出来，看数据表现是否正常

## Redis

### Redis 为什么用SDS

simple data structure结构如下

``` c
struct  sdsher{
//记录buf中已保存字符的长度
//等于SDS所保存的字符串的长度
int  len;
//记录buf数组中未使用字节的数量
int free;
//字节数组，用于保存字符串
char buf[];
};
```

原因：

- 常数复杂度获取字符串长度
- C语言中实现字符串长度计数的复杂度为O（N）。
  而反观SDS中，因为其结构中保存了len这一属性，所以获取SDS长度的复杂度仅为O（1）。

### Redis 的持久化策略，采用的哪种

aof（记录操作日志），rdb（dump 文件）

### Redis 的主从复制，哨兵，集群

### Redis 的数据类型

string，list，set，hash，zset

## Linux 

### Linux 中 将 文本 第二列 提取出来 再排个序

awk 取出第二行，通过管道符传给sort 去排

### awk 和 sed 区别

awk是一种**程序语言**，awk擅长从格式化报文或从一个大的文本文件中抽取数据。

sed 是一个精简的、**非交互式的编辑器**，跟vim 是一个概念。

1. 如果文件是格式化的，即由分隔符分为多个域的，优先使用awk
2. awk适合按列（域）操作，sed适合按行操作
3. awk适合对文件的抽取整理，sed适合对文件的编辑。

### Linux 进程调度 算法

- 先来先服务（FCFS)调度算法

- 短作业优先（SJF）调度算法

- 优先调度算法

- 时间片轮转调度算法

  https://blog.csdn.net/qq_43414142/article/details/90676984

## Java及编程基础

### Java 服务 检测性能并调优

1. top 命令确认哪个进程有问题，并观察是 cpu 问题，还是 内存 问题，(io问题的话用 iotop)

2. -  pidstat 查询进程下所有线程的cpu,内存情况，拿到有问题的 线程id，（io 用

     ```
      pidstat -d
     ```

      查询）

   - jstack 打印堆栈信息，并将 线程id 转为16进制 ，然后查询（找到cpu 偏高的代码段）

   -  jmap 查看是否一直频繁创建重复对象（内存问题）

3. **cpu 偏高** 一般是因为 死锁，代码计算逻辑复杂

   **内存偏高** ：一直创建对象导致频繁GC（可以用jmap 命令排查）

   **io 偏高** ：数据库索引失效，拉取数据慢。数据量过大（加缓存）

### HashMap 底层原理

数组+链表+红黑树

链表节点超过8 -》红黑树

红黑树节点小于6 -〉 链表

扩容： 底层会创建一个长度为16的数组(Node类型)，加载因子为0.75，当我们向集合中添加的数据超过12 （16 * 0.75）时便会扩容，扩容为原来的2倍。

**为什么是16**：用hash值与n-1进行&运算，结果就是该数据存储的位置----15（16-1）的二进制为0000 1111，31（32-1）的二进制为 0001 1111，

​		  如果最后几位全部是1的话那么hash值是多少都能得到

​		  （数组上的每一个索引位置都可能算到，否则一定有些索引位置将永远无法得到，也就是将永远无法存数据，造成空间浪费）

### 线程的五种状态及切换

![image-20211007202214023](/Users/sun9/IdeaProjects/library/picture/image-20211007202214023.png)

进入Runnable状态大体分为5种：

- 线程调用sleep()方法经过的时间超过了指定的时间。
- 线程正在等待某个通知，其他线程发出了通知。
- 处于挂起的线程调用resume()方法。
- 线程调用的阻塞IO已返回，阻塞方法执行完毕。
- 线程成功的获取到了同步监视器。

出现Blocked的情况大概分为5种

- 线程调用sleep()方法，主动放弃占用的CPU资源。
- 线程调用wait()方法，等待某个通知。
- 线程调用suspend()方法将线程挂起，容易导致死锁，尽量避免使用此方法。
- 线程调用阻塞式IO方法，在方法返回前，线程被阻塞。
- 线程试图获得一个同步监视器，但该同步监视器被其他线程所持有。

### 进程和线程的区别

进程是操作系统资源分配的基本单位，而线程是CPU调度和分派的基本单位

进程有自己独立的地址空间，每启动一个进程，系统都会为其分配地址空间，建立数据表来维护代码段、堆栈段和数据段，线程没有独立的地址空间，它使用相同的地址空间共享数据；

内存方面有点类似于容器与task的区别，进程有一个共享工作内存，线程可以使用这个内存，但是线程与线程之间不能互相使用它们的工作内存。

**进阶：**这虽然看着很像slot 和TM 的区别，但是slot 不是线程，而是对象。

### 并行和并发的区别

你吃饭吃到一半，电话来了，你停了下来接了电话，接完后继续吃饭，这说明你支持并发。
你吃饭吃到一半，电话来了，你一边打电话一边吃饭，这说明你支持并行。

并发的关键是你有处理多个任务的能力，不一定要同时。
并行的关键是你有同时处理多个任务的能力。

所以我认为它们最关键的点就是：是否是『同时』。



### 堆和栈区别

**数据结构**

- 堆：堆是满足父子节点大小（比如大根堆中规定父节点的值要比子节点大）关系的一种完全二叉树
- 栈：。。

**操作系统**

- 堆：线程共享
- 栈：线程独占

### G1 跟最新的收集器ZGC有什么区别

G1打破了以往将收集范围固定在新生代或老年代的模式，GI 将 Java 堆空间分割成了若干相同大小的 区域，即 region，包括 Eden、Survivor、 Old、 Humongous 四种类型。

和G1类似，但ZGC的region的大小更加灵活和动态。zgc的region不会像G1那样在一开始就被划分为固定大小的region。

zgc的region核心亮点就是：动态。

### 创建线程的方式

- **定义一个类继承Thread类，并重写Thread类的run()方法**
  - 不能共享成员变量
  - 线程的执行是抢占式
- **通过实现Runnable接口创建线程类**
  - 可以使多线程共享线程类的实例变量
  - 线程类只是实现了接口，还可以继承其他类；
  - 需要注意**线程安全问题**
- **通过Callable和Future接口创建线程**
  - Runnable的增强版，有返回值，能处理异常
- **使用Executor框架来创建线程池**

### Java JUC 包下的类

-  ReenTrantLock(可重入锁）

-  semaphore（信号量）用于限流

-  CountDownLatch-计数器 

  当全部线程进行到特定的位置后，才执行下一次的操作-只能使用一次
  创建时后一个计数器，每次的CountDownLatch没让计数器减1，直到为0，执行下一步操作
  CountDownLatch.await()和CountDownLatch.countdow

- 最重要的基本类 AQS

### 提交线程往一个线程池提交线程，如果发现线程池满了，就阻塞提交线程，等线程池空闲了在继续执行，这个功能如何实现？生产消费模型

实现生产消费模型一般有三种方法

-  在synchronized 修饰的方法中调用 wait和notify 
- 在 使用lock 的情况下，调用 condition 的 await、signalAll
- 使用 BlockingQueue  put和take，底层也是 lock 的逻辑，不过是Juc 包下已经实现好的类

https://www.cnblogs.com/twoheads/p/10137263.html

### 创建线程池的参数

- corePoolSize：线程池的大小
- maximumPoolSize：线程池中创建的最大线程数
- keepAliveTime：空闲的线程多久时间后被销毁
- workQueue：阻塞队列，用来存储等待执行的任务，决定了线程池的排队策略，有以下取值：
  　　ArrayBlockingQueue;
    　　LinkedBlockingQueue;
    　　SynchronousQueue;

### 创建线程池的方式

①. newFixedThreadPool(int nThreads)

创建一个固定长度的线程池，每当提交一个任务就创建一个线程，直到达到线程池的最大数量，这时线程规模将不再变化，当线程发生未预期的错误而结束时，线程池会补充一个新的线程。

②. newCachedThreadPool()

创建一个可缓存的线程池，如果线程池的规模超过了处理需求，将自动回收空闲线程，而当需求增加时，则可以自动添加新线程，线程池的规模不存在任何限制。

③. newSingleThreadExecutor()

这是一个单线程的Executor，它创建单个工作线程来执行任务，如果这个线程异常结束，会创建一个新的来替代它；它的特点是能确保依照任务在队列中的顺序来串行执行。

④. newScheduledThreadPool(int corePoolSize)

创建了一个固定长度的线程池，而且以延迟或定时的方式来执行任务，类似于Timer。

### hash table ，vector为什么线程安全

关键方法用了 synchronized 修饰

### ConcurrentHashMap是如何实现线程安全的，为什么效率高

用了很多 CAS 和 volatile 来规避加锁，即使加锁，也只是锁hash冲突的那个位置，其他位置仍然支持并发，效率很高，而且synchronized在1.8 做了优化，有锁的状态升级

https://blog.csdn.net/qq_41737716/article/details/90549847

### 如何保证线程安全

线程安全就是多线程访问同一代码，不会产生不确定的结果。

衡量线程是否安全, 主要从三个特性入手

- 原子性：是指一个操作或多个操作要么全部执行，且执行的过程不会被任何因素打断，要么就都不执行。
- 可见性：当一个线程修改了线程共享变量的值，其它线程能够立即得知这个修改，如果只为了保证可见性，可以使用**volatile**关键字
- 有序性：即程序执行的顺序按照代码的先后顺序执行。

方法有很多：TreadLocal,加锁，可重入代码。

ThreadLocal与像synchronized这样的锁机制是不同的。首先，它们的应用场景与实现思路就不一样，锁更强调的是如何同步多个线程去正确地共享一个变量，ThreadLocal则是为了解决同一个变量如何不被多个线程共享。从性能开销的角度上来讲，如果锁机制是用时间换空间的话，那么ThreadLocal就是用空间换时间。

**可重入代码：**完全不引用外部变量，所以不管多少线程来执行几次，都是同样的结果

Spring 的dao,service 用的就是TreadLocal，ThreadLocal 是线程本地变量，每个线程拥有变量的一个独立副本，所以各个线程之间互不影响，保证了线程安全

### 锁的分类

- lock：接口，需要手动释放,当一个线程处于等待某个锁的状态,可以手动中断

- synchronized：关键字，自动释放,当一个线程处于等待某个锁的状态，是无法被中断的，只有一直等待下去

- CAS(无锁) ：只能保证原子性，乐观锁的一种，不会阻塞任何线程。CAS有3个操作数,内存值V, 旧的预期值A,要修改的新值B.当且仅当预期值A和内存值V相同时, 将内存值V修改为B,否则什么都不做.**但是可能引发 ABA问题**（如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了），解决思路是用版本号来控制

  举例：AtomicInteger

- 可重入锁：所谓可重入锁,指的是以线程为单位,当一个线程获取对象锁之后,这个线程可以再次获取本对象上的锁，**防止死锁**

  synchronized 和 ReentrantLock 都是可重入锁，实现原理是基于AQS

  ReentrantLock 默认构造是**非公平锁**

### 公平锁 和非公平锁

**公平锁：**多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列的第一位才能得到锁。

- 优点：所有的线程都能得到资源，不会饿死在队列中。
- 缺点：吞吐量会下降很多，队列里面除了第一个线程，其他的线程都会阻塞，cpu唤醒阻塞线程的开销会很大。

**非公平锁**：多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。

- 优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点，CPU也不必取唤醒所有线程，会减少唤起线程的数量。
- 缺点：可能导致队列中间的线程一直获取不到锁或者长时间获取不到锁，导致饿死。

### lock 和 synchronized区别

- 首先synchronized是java内置关键字，在jvm层面，Lock是个java类；
- synchronized无法判断是否获取锁的状态，Lock可以判断是否获取到锁；
- synchronized会自动释放锁(a 线程执行完同步代码会释放锁 ；b 线程执行过程中发生异常会释放锁)，Lock需在finally中手工释放锁（unlock()方法释放锁），否则容易造成线程死锁；
- 用synchronized关键字的两个线程1和线程2，如果当前线程1获得锁，线程2线程等待。如果线程1阻塞，线程2则会一直等待下去，而Lock锁就不一定会等待下去，如果尝试获取不到锁，线程可以不用一直等待就结束了；
- synchronized的锁可重入、不可中断、非公平，而Lock锁可重入、可判断、可公平（两者皆可）；
- Lock锁适合大量同步的代码的同步问题，synchronized锁适合代码少量的同步问题。

### AQS

AQS维护了一个volatile的int 类型的state来表示加锁的状态，和一个CLH（FIFO）双向队列，用来存储获取锁失败线程的队列，还有变量**exclusiveOwnerThread**，它表示的是获得锁的线程，也叫独占线程。

过程图

![image-20211006214227189](/Users/sun9/IdeaProjects/library/picture/image-20211006214227189.png)



### synchronized 锁状态升级

**无锁**

无锁是指没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。

**偏向锁**

偏向锁是指当一段同步代码一直被同一个线程所访问时，即不存在多个线程的竞争时，那么该线程在后续访问时便会自动获得锁，从而降低获取锁带来的消耗，即提高性能。

偏向锁在 JDK 6 及之后版本的 JVM 里是默认启用的。可以通过 JVM 参数关闭偏向锁：-XX:-UseBiasedLocking=false，关闭之后程序默认会进入轻量级锁状态。

**轻量级锁**

轻量级锁是指当锁是偏向锁的时候，却被另外的线程所访问，此时偏向锁就会升级为轻量级锁，其他线程会通过自旋（关于自旋的介绍见文末）的形式尝试获取锁，线程不会阻塞，从而提高性能。

若当前只有一个等待线程，则该线程将通过自旋进行等待。但是当自旋超过一定的次数时，轻量级锁便会升级为重量级锁（锁膨胀）。

另外，当一个线程已持有锁，另一个线程在自旋，而此时又有第三个线程来访时，轻量级锁也会升级为重量级锁（锁膨胀）。

**重量级锁**

重量级锁是指当有一个线程获取锁之后，其余所有等待获取该锁的线程都会处于阻塞状态。

http://www.jetchen.cn/synchronized-status/

### 分布式锁

https://blog.csdn.net/hanliuxi4265/article/details/87971982

https://blog.csdn.net/wuzhiwei549/article/details/80692278



### 递归转为非递归

用栈来模拟递归过程

### 正向代理和反向代理的区别

正向代理 是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。

反向代理正好相反，对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理 的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容 原本就是它自己的一样。

**区别**：正向代理即是客户端代理, 代理客户端, 服务端不知道实际发起请求的客户端.反向代理即是服务端代理, 代理服务端, 客户端不知道实际提供服务的服务端

### https 用了什么加密

> 目前常见的加密算法有：DES、AES、IDEA 等
>
> 目前常见非对称加密算法：RSA，DSA，DH等。

非对称加密+对称加密

1. 某网站拥有用于非对称加密的公钥A、私钥A’。
2. 浏览器向网站服务器请求，服务器把公钥A明文给传输浏览器。
3. 浏览器随机生成一个用于对称加密的密钥X，用公钥A加密后传给服务器。
4. 服务器拿到后用私钥A’解密得到密钥X。
5. 这样双方就都拥有密钥X了，且别人无法知道它。之后双方所有数据都通过密钥X加密解密即可。

### Java 设计模式

单例 ：构造器私有化

工厂 （flink 源码）

模版 ： 抽象类定义模版方法，某些 抽象方法交给继承类实现（AQS源码）

### 对云服务的存储计算分离怎么理解

经过 10 年的发展，网络的性能发生了巨大的变化，从之前主流 100Mb 到 10Gb，增长了100倍，而同时期的 HDD 硬盘的性能基本没有太大变化，倒是单盘的容量增大了很多。**大数据的瓶颈逐渐由 IO 变成了 CPU**

而因为存储和计算耦合在一个集群中，带来了一些其它问题：

1. 在不同的应用或者发展时期，需要不同的存储空间和计算能力配比，使得机器的选型会比较复杂和纠结；
2. 当存储空间或计算资源不足时，只能同时对两者进行扩容，导致扩容的经济效率比较低（另一种扩容的资源被浪费了）；
3. 在云计算场景下，不能实现真正的弹性计算，因为计算集群中也有数据，关闭闲置的计算集群会丢失数据。

而存储计算分离恰好能解决这些问题

### 面向对象和面向函数的区别

举个做饭的例子

面向函数：1.先放水，2.再放米，3.通电。。。

面向对象：创造个机器人，设定好做法的程序，要做饭的时候启动即可

主要想说明的就是 

- **面向函数**就是分析出解决问题所需要的步骤，然后用函数把这些步骤一步一步实现，使用的时候一个一个依次调用就可以了
- **面向对象** 就是 万事万物皆对象，把问题看作对象的属性和行为，以对象为中心思考解决问题

### JVM 执行模式

- 解释模式：只使用解释器（-Xint 强制JVM使用解释模式），执行一行JVM字节码就编译一行为机器码。
  - 特点：启动快，但执行慢，适用于大多数代码只会执行一次的情况
- 编译模式：只使用编译器，先将所有的JVM字节码一次编译为机器码，然后一次性执行所有机器码。
  - 特点：启动慢，执行快，适合代码反复执行的场景
- （默认）混合模式：起始阶段采用解释模式，后续会有一个热点代码监测（HotSpot），对热点代码进行编译
  - 热点代码监测主要是看有没有调用多次的方法或者循环

### controller 是不是单例的

spring 默认就是单例，因为这样快，不会每次使用都要去new一个对象，还可以节省空间



### LSM 树

LSM树核心思想的核心就是放弃部分读能力，换取写入的最大化能力。LSM Tree ，这个概念就是结构化合并树的意思，它的核心思路其实非常简单，就是**假定内存足够大**，因此不需要每次有数据更新就必须将数据写入到磁盘中，而可以先将最新的数据驻留在内存中，等到积累到足够多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾(因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起)。

### TCP UDP区别

![image-20210909233402696](/Users/sun9/IdeaProjects/library/picture/image-20210909233402696.png)

核心区别就是 ：

- TCP是传输控制协议，是面向连接的通讯协议（如：打电话）
- UDP是用户数据报协议，是面向无连接的通讯协议（如：发短信）

TCP ：点对点，类似于Kafka的ack =-1

UDP： 广播，类似于Kafka的 ack=1

https://blog.csdn.net/weixin_45372436/article/details/100357832

### RPC 和HTTP的区别

主要用于服务间的远程调用

RPC：自定义数据格式，基于原生TCP通信，速度快，效率高，缺点是客户端和服务端需要统一框架和语言

HTTP：规定了数据传输的格式，缺点是消息封装臃肿，优点是无需关注语言的实现，开发很灵活



### TCP 三次握手四次挥手

握手：

![image-20210830111849700](/Users/sun9/IdeaProjects/library/picture/image-20210830111849700.png)

首先Client端发送连接请求报文，Server段接受连接后回复ACK报文，并为这次连接分配资源。Client端接收到ACK报文后也向Server段发生ACK报文，并分配资源，这样TCP连接就建立了。

挥手：

![image-20210830113620023](/Users/sun9/IdeaProjects/library/picture/image-20210830113620023.png)

【注意】中断连接端可以是Client端，也可以是Server端。

假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说"我Client端没有数据要发给你了"，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，"告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息"。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，"告诉Client端，好了，我这边数据发完了，准备好关闭连接了"。Client端收到FIN报文后，"就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，"就知道可以断开连接了"。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！
————————————————
版权声明：本文为CSDN博主「whuslei」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/whuslei/article/details/6667471

### 如何设计 消息队列

消息队列 核心就是三大组件，生产者，服务端，消费者，这个服务端的作用就是把一次rpc 转化为 2次 rpc，还有消息的存储。

针对消息队列的特性选取相应的组件

- rpc 的一些负载均衡，序列化协议： 这些已经有很成熟的开源框架来支撑了，没必要重复造轮子，直接借用 阿里 的 dubbo
- 高可用：因为rpc 的框架支持高可用，所以消息是至少一次的，所以只要选择一个合适的分布式的kv存储，就可以了
- 速度： 理论上，从速度来看，文件系统>分布式KV（持久化）>分布式文件系统>数据库，而可靠性却截然相反。分布式KV（如MongoDB，HBase）等，或者持久化的Redis，由于其编程接口较友好，性能也比较可观，如果在可靠性要求不是那么高的场景，也不失为一个不错的选择。

### 红黑树 和  AVL树（平衡二叉搜索树） 区别

二者都是**平衡二叉树**，但是区别如下：

1. 红黑是用非严格的平衡来换取增删节点时候旋转次数的降低，任何不平衡都会在三次旋转之内解决，而AVL是严格平衡树，因此在增加或者删除节点的时候，根据不同情况，旋转的次数比红黑树要多。
2. AVL树提供比红黑树更快的查找，因为avl树 严格平衡
3. AVL树存储每个节点的平衡因子或高度，因此每个节点需要存储一个整数，而红黑树每个节点只需要1位信息。
4. 实际应用中，若搜索的次数远远大于插入和删除，那么选择AVL，如果搜索，插入删除次数几乎差不多，应该选择RB。

### 排序比较

![image-20211012132229783](/Users/sun9/IdeaProjects/library/picture/image-20211012132229783.png)

## 问问题

数据开发跟数据分析的协作模式是这么样的？是全链路的开发还是

公司内部的技术文档有没有规范，因为现在我这个公司虽然有文档，但是都没人遵守规范，形同虚设。
