## 自我介绍

面试官好，我之前是在杭州的一家跨境电商担任数据开发工程师，期间经历的项目有离线数仓和基于flink的实时分析平台，去年7月份来的深圳，也就是现在的公司任职至今。在这个公司数仓也做过，大数据组件平台开发也做过。可以说不管是业务上层还是架构底层都有所涉及。目前是因为公司架构调整，个人感觉对自己的职业规划有影响，所以打算换个平台继续发展。

## 项目介绍

### 平台

​	之前公司抽数用的是第三方的软件，但是存在支持数据源少，对数据库负载高的问题，为了解决这个问题，公司决定划分平台组来开发自研的抽数平台。但是我是有点自己的想法的，之前我有看过网易有数那套平台的相关文章，我就想其实我们要做的产品，跟这个是相似的。所以借鉴了目前市面上成熟的平台的一些经验，开发了元数据平台，抽数平台，任务监控平台，任务部署平台，统一计算引擎，支持配置多集群，多数据源，多张表,经过测试，吞吐量能达到每分钟千万级。接下来我简单讲下各个模块的原理：

- 抽数平台：利用debezium 作为一个插件，配合Kafka connect 来将业务库里的数据都发往Kafka中，之后通过Flink SQL 来写到对应的数据源（kudu，upsert_kafka等），值得一提的是，在往kudu里写的时候发现目前开源的connector使用起来有问题，不支持撤回流写入（识别不了主键），时态表等功能，我们自己改了源码，提供了这些功能。
- 元数据平台：在用户选择source表时，会从对应数据源拉取对应的元数据到我们平台的元数据库，然后根据用户选择的输出源进行自动的字段映射，同时也维护一份输出表元数据，而维护的这些元数据通过自动拼接DDL来在抽数平台和SQL开发平台上实现无DDL编程。
- 任务监控平台：基于zookeeper的监听器机制，每次任务启动时去注册一个临时节点，并启动监听器，一旦任务失败则会根据报警策略报警。
- 任务部署：目前只支持flink on yarn 部署，后续打算将spark，k8s集成进去。用户在前端传入相应的运行配置，抽数平台只需要点点点，sql开发平台需要输入transform SQL，然后把这些配置序列化存到redis配置中心中。其部署任务那块，主要参考flink-yarn 模块，先通过传入flink的配置（运行资源，引擎jar包），yarnClient,yarn conf ，yarn回调函数，生成集群描述器，然后调用部署方法即可完成部署
- 统一计算引擎：有点类似于zeppelin，分source，transform，sink，**目的是使用同一套代码，最大限度的去兼容不同的计算引擎，集群环境**。基于SQL开发环境，flink ，spark各包含一个main方法，主要工作就是读取配置中心，封装成方法入参，底层还是使用的env.excutesql（），不过为了避免一个表生成一个job，采用了StatmentSet,把任务都封装为了一个job，最大化节约资源。

### 云数仓

整体架构是 

- 数据采集： dataworks,http 请求，sdk,DTS
- 计算引擎： maxcompute,vvp
- 存储： oss，hologres，kafka，Maxcompute（盘古）
- 调度： Maxcompute（伏羲）
- 即系查询： Holoweb

因为计算用的都是阿里云的云上资源，能够更好的进行资源的利用。

采用的是lamda 架构，批处理和流处理分开计算，为了保证数据的最终一致性，每天会有一个同步任务将批处理的数据刷写到流处理的表中。

​	亮点：

1. 公司有时候会有一些对历史数据状态复用的需求场景（学习资源停留时长），这里我用到的方案是将需要的历史数据加载到内存，同时跟实时流union 再group by ，然后取last_value(防止离线和实时数据结合后乱序)，这样就可以得到一条含有历史状态的撤回流。

2. 用户更改id时，需要将之前的数据也全部更改为最新的id，用到的是 自定义filter 算子，（所有事件都是来自于同一个topic），检测到 用户更改id 事件，则直接发 jdbc update 请求，

这个架构也有一个缺点，就是过于黑盒，比如登录不了服务器，查看不了进程gc状态，没有办法灵活的使用JVM参数进行一些调优。

### 离线数仓

之前公司数仓存在的问题主要有：

1. SQL冗余：每个宽表都是从ods直接取的，没有中间的建模过程，导致一个SQL有700多行是常有的事，很难维护。
2. 小文件：使用Data pipeline实时同步数据时，会产生大量小文件
3. 建表DDL与业务逻辑SQL混在一起，不易维护

所以参照领域驱动模型的一些思想，有以下几层结构：

1. driver：方法入口
2. Service: 具体的转化逻辑
3. repostity:通用方法，比如读取数据，写入数据
4. DDL：主要存放表的DDL，并且会维护一个JSON文件，存了表名和DDL文件的映射，到时候会根据表名去获取DDL语句来进行创建表的操作

同时我们也写了一个合并小文件的脚本，原理就是读出来写入到临时表，然后写回到表里,控制reduce个数为2；（支持动态传入）

总体建模有四层：source（主要针对实时同步的表，会含有DDL标识），ods，dwd，dim，DM

总体的数据流向：

- 业务库：polarDB，PG
- 抽数中间件：sqoop，dataWorks,datapipline
- 数仓：Hive
- 宽表交互：ADBPG

**离线数仓重构的效果有什么量化指标来衡量**？

## 平安

### Flink submit on yarn 的API改造

查看flink-yarn 源码，发现flink on yarn 的部署都是先createYarnClusterDescriptor，通过传入flink的配置（hadoop jars,flink jars,flink 总的运行配置（最大并行度），引擎jar包），yarnClient,yarnConf(加载Hadoop conf,

```
// 设置顶层hdfs实现为DistributedFileSystem
configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
```

),yarn回调函数（可查看集群信息），然后调用deployApplicationCluster（运行配置（内存，核心数），applicationName），即可完成部署。

### 计算引擎jar包如何设计

​	基于Flink的设计思想，有Source，Transform，Sink阶段

- Source: 执行映射表的逻辑
- Transform: 利用StatamentSet,将多个执行SQL糅合为一个任务，节约资源
- Sink: 维护了一个catalog集合，根据输出数据源动态加载catalog去执行DDL操作

PS：想利用Spirngboot 创建对象是单例的特性，但是发现1.9版本的flink跟 SpringBoot整合有问题，俩个运行的不在同一个JVM上，会找不到对象，而且也不想引多余的jar包，所以自己实现了一下IOC来自动注入。

### 多个表抽数的时候，如果一个表出错，怎么恢复

只能重跑该任务下的所有表，但是表并不会很多，而且从维护了检查点，从检查点恢复也不会重跑很多数据。

- 任务量：一个任务最多只能包含一个database下10个小表，或者一个database下的5个大表。

### flink 检查点对齐原理

分布式快照，将检查点的保存和数据处理分离开：

1. jobManager 会向source 发送一个带有新checkpoint ID 的信息，以此行为开启检查点
2. source 接收到信息后 会先将状态存入状态后端，然后通知jobmanager完成，接着开始广播checkpoint barrier.
3. 下游算子会等待所有分区都接收到barrier，已经接收到barrier的分区会将接下来的数据缓存起来，而未接收到的barrier的分区则做正常的数据处理，当所有分区都接收到barrier的时候就会触发检查点的保存，通知jobmanager,然后将barrier往下游发送。
4. 当Sink 端也向job manager 确认完毕后，本次checkpoint 完成。

这里得提一下，当任务将其状态保存到检查点时，它其实处于一个阻塞状态，而状态又可能比较大，有可能会阻塞个几分钟，所以我们选用的是支持异步快照的RocksDB，它会创建一个本地的副本，然后开启另一个线程去复制到远程存储，减少了任务的等待时间。



### flink checkpoint 什么情况会失败

先打一套检查点对齐原理----

在这中间的任何环节都可能会失败，但是在生产中最常碰见的就是数据倾斜导致的checkpoint超时

- 扯到生产上碰到的计算一个小时内每个地区的PV，最开始采用的方法是直接开窗再sum，然后发现检查点失败，某几个task还出现了反压，后来的解决思路：按照（地区+随机数）keyby之后开窗，然后用聚合算子aggregate，然后再keyby（去除随机数），聚合，算出最终值

### 状态特别大如何解决（计算一个小时PV）？窗口如何设计？

代码逻辑层面：

- 增量聚合与全量聚合搭配使用

运行配置层面：

- 状态大可以开启异步快照（默认都是开启的），
- 使用ROCKsDB的话还可以开启增量快照，
- 同时也可以增大检查点之间的间隔，避免浪费过多的资源在检查点保存上

### flink 源码级别的BUG诊断

当使用Kudu- connector往Kudu中写数时，发现不支持撤回流写入，跟源码发现他内部走的是老的API,输出用的是DataStreamSink，在SQL中指定的主键不生效，后来自己定义了Kudu的动态数据源DynamicTableSink，使用的输出是SinkFunctionProvider，并且在工厂生产之前传入了shchema信息，以便后续做key的校验（参考JDBCSINk）打通了kafka到kudu的撤回流写入功能。

### HIVE SQL 的执行过程

Hive和spark 的Sql解析器是使用的Antlr4，但是优化过程也借用了Calcite的基于代价的优化策略，flink用的是Calcite。

- 基于代价的优化策略，会单独起一个任务，代价衡量值是cpu和IO，计算各个Opertator Tree节点的代价总和，从而得到代价最小的执行计划

根据Hive 的架构图说，

![preview](/Users/sun9/IdeaProjects/library/picture/view.png)

![preview](/Users/sun9/IdeaProjects/library/picture/view1.png)

SQL解析过程：

1. Parser：将sql解析为AST（抽象语法树），会进行语法校验，AST本质还是字符串。
2. Analyzer：语法解析，生成QB（query block）
3. 逻辑执行计划解析，生成一堆Opertator Tree
4. 进行逻辑执行计划优化，生成一堆优化后的Opertator Tree
5. 物理执行计划解析，生成tasktree
6. 进行物理执行计划优化，生成优化后的tasktree，该任务即是集群上的执行的作业

**执行过程**

1. 用户提交查询等任务给Driver。

2. 编译器获得该用户的任务Plan。

3. 编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息。

4. 编译器Compiler得到元数据信息，对任务进行编译，先将HiveQL转换为抽象语法树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理的计划（MapReduce）, 最后选择最佳的策略。

5. 将最终的计划提交给Driver。

6. Driver将计划Plan转交给ExecutionEngine去执行，获取元数据信息，提交给JobTracker或者SourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作。

7. 获取执行的结果。

8. 取得并返回执行结果。
————————————————
版权声明：本文为CSDN博主「wuyue_fighting」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_39093097/article/details/99690604

### HBASE REGION 的Rebalance 触发条件

- 触发条件：
  - 自动触发：如果开启balance_switch 参数，在HMaster中，后台会起一个线程定期检查是否需要进行rebalance，线程叫做BalancerChore。线程每隔 hbase.balancer.period会定期执行 master.balance()函数，配置项默认300000毫秒，5分钟。每次balance最多执行hbase.balancer.max.balancing，如果没有配置，则使用hbase.balancer.period配置项的值。
  - 手动触发：balancer
- 负载均衡原理：

1. 计算均衡值的区间范围，通过总Region个数以及RegionServer节点个数，算出平均Region个数，然后在此基础上计算最小值和最大值

   ```
   # hbase.regions.slop 权重值，默认为0.2
   最小值 = Math.floor(平均值 * (1-0.2))
   最大值 = Math.ceil(平均值 * (1+0.2))
   ```

2. 遍历超过Region最大值的RegionServer节点，将该节点上的Region值迁移出去，直到该节点的Region个数小于等于最大值的Region

3. 遍历低于Region最小值的RegionServer节点，分配集群中的Region到这些RegionServer上，直到大于等于最小值的Region

4. 负责上述操作，直到集群中所有的RegionServer上的Region个数在最小值与最大值之间，集群才算到达负载均衡，之后，即使再次手动执行均衡命令，HBase底层逻辑判断会执行忽略操作

## 极光

### Kafka如何保证数据的稳定性

- ISR机制
- ack应答机制
- 故障处理：HW、LEO

### Kafka partition 消费堵塞原因分析（感觉像问的是消费能力不足）

1）如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）

2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间<生产速度），使处理的数据小于生产的数据，也会造成数据积压。

### Kafka partition 和消费者数量关系

partition 数量 >= consumer 数量

再讲一下消费策略：

range - 首先会统计一个消费者，一共订阅了哪些主题！以主题为单位，  根据     主题的分区数 / 当前主题订阅的消费者个数 ，根据结果，进行范围的分配！

Robin - 以主题为单位，将主题的分区进行排序，排序后采取轮询的策略，将主题轮流分配到订阅这个主题的消费者上！

sticky- 也是轮询的方式，不过rebalance 时会保证还存在的消费者已分配的分区不动。

### 数据去重方式，以及优缺点

- distinct ：对select 后面所有字段进行去重。会把所有数据发到一个reduce里面，效率较低
- Group  by: 对group by 后的字段进行去重，会有多少组发送给多少个reduce，效率高
- Row_number:语法上比group by更加灵活，但是有个排序的过程，效率稍低。

### map ,map partition 的区别

- map是对rdd中的每一个元素进行操作；一个function要执行的次数等于元素的个数

- mapPartitions则是对rdd中的每个分区的迭代器进行操作，把一个分区的数据都收集起来进行消费，一个分区只执行一次function，但是数据量大的时候就会OOM。

### spark OOM 怎么分析以及解决方法

​	首先需要了解Spark 的内存管理机制，他主要分为三块，缓存内存，计算内存，其他内存（存数据结构，元数据等信息），缓存内存和计算内存之间是动态管理的。

​	接下来在说一下OOM发生的地点，Driver端和Excutor 端。

Driver ： 1. 定义了一个很大的变量（缓存）

​				2.把所有数据拉到driver 处理（缓存）

​				3.spark UI 也会占据一部分内存（计算）

解决方案：加内存

Excutor:

- 1.广播了一个大变量到blockmanager（缓存）

- 2.数据倾斜（计算）

- 3.reduce缓冲区过大（计算）

解决方案：加内存-》针对数据倾斜：1.提高shuffle并行度2.先过滤

3.加盐4.join 类（mapjoin ， 稀释扩容）

### Hbase rowKey设计

原则：长度（不要过长，会影响检索效率），散列，唯一

主键+hash

### Flink 保证精准一次性

- Source:可重设数据读取位置

- Transform:检查点一致性算法

- Sink：

  - 事务性写入：

    - 俩阶段提交：

      1. 请求阶段：协调者向每个参与者发送事务请求，当参与者完成事务后会返回给协调者OK信号，如果都返回的是OK，则进入下一个阶段，否则进行回滚。
      2. 提交阶段：协调者向参与者发起事务提交通知，参与者提交事务，并释放资源

      存在问题：

      1. 单点故障：由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。
      2. 同步阻塞：执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

  - 幂等性写入

### HashMap 底层原理

数组+链表+红黑树

链表节点超过8 -》红黑树

红黑树节点小于6 -〉 链表

扩容： 底层会创建一个长度为16的数组(Node类型)，加载因子为0.75，当我们向集合中添加的数据超过12 （16 * 0.75）时便会扩容，扩容为原来的2倍。

### 进程和线程的区别

进程是操作系统资源分配的基本单位，而线程是CPU调度和分派的基本单位

进程有自己独立的地址空间，每启动一个进程，系统都会为其分配地址空间，建立数据表来维护代码段、堆栈段和数据段，线程没有独立的地址空间，它使用相同的地址空间共享数据；

内存方面有点类似于容器与task的区别，进程有一个共享工作内存，线程可以使用这个内存，但是线程与线程之间不能互相使用它们的工作内存。

**进阶：**这虽然看着很像slot 和TM 的区别，但是slot 不是线程，而是对象。

### kudu 和Hbase的异同

- 不同：

1. Hbase依托于zookeeper，利用其来存储元数据表位置，master位置，RegionServer的工作状态，而Kudu将这些工作都交给了Master
2. Hbase将数据持久化这部分的功能交给了Hadoop中的HDFS，最终组织的数据存储在HDFS上。依靠 HDFS保证数据可靠性。Kudu自己将存储模块集成在自己的结构中，内部的数据存储模块通过Raft协议来保证leader Tablet和replica Tablet内数据的强一致性，和数据的高可靠性。为什么不像HBase一样，利用HDFS来实现数据存储，猜测可能是因为HDFS读小文件时的时延太大，所以Kudu自己重新完成了底层的数据存储模块，并将其集成在TServer中。
3. hbase列族存储，kudu列式存储
4. 针对相同主键更新操作的数据，Hbase是允许多版本的数据存在的，所以直接当做一条新插入的数据，而kudu只允许有一条存在，所以会将更新操作单独存放于内存，等待刷新或读取的时候再合并。

- 相同：

1. 都是通过数据的timestamp字段来实现mvcc，但是**hbase可以手动指定，kudu不能**

### Kudu 的调参优化

### Hbase为什么采用列族存储

因为Hbase底层存储依托于HDFS，如果行式存储不适用于做olap查询，而列式的话，因为一列一个文件，可能会有小文件的问题，所以采用了列族存储（个人观点）

### 为什么Kudu 流式写入性能为什么能这么高，里面有批处理的逻辑吗

主要得益于LSM树存储引擎，先顺序写到wal文件中

### Spark 和MR的区别

1. **基于内存数据处理模式**

   spark是基于内存进行数据处理的，MapReduce是基于磁盘进行数据处理

   的

2. **DAG计算模型**

   Spark 计算比 MapReduce 快的根本原因在于 DAG 计算模型。一般而言，

   DAG 相比MapReduce 在大多数情况下可以减少 shufflfflffle 次数。Spark 的 

   DAGScheduler 相当于一个改进版的 MapReduce，如果计算不涉及与其

   他节点进行数据交换，Spark 可以在内存中一次性完成这些操作，也就是

   中间结果无须落盘，减少了磁盘 IO 的操作。但是，如果计算过程中涉及数

   据交换，Spark 也是会把 shufflfflffle 的数据写磁盘的！

3. **资源申请粒度**

   spark是粗粒度资源申请，也就是当提交spark application的时候，

   application会将所有的资源申请完毕，如果申请不到资源就等待，如果申

   请到资源才执行application，task在执行的时候就不需要自己去申请资

   源，task执行快，当最后一个task执行完之后task才会被释放。

   **优点是执行速度快，缺点是不能使集群得到充分的利用**

   MapReduce是细粒度资源申请，当提交application的时候，Task执行时，

   自己申请资源，自己释放资源，task执行完毕之后，资源立即会被释放，

   task执行的慢，application执行的相对比较慢。

   优点是集群资源得到充分利用，缺点是application执行的相对比较慢。

   **Spark是基于内存的，而MapReduce是基于磁盘的迭代**

### 说一说Shuffle 的理解

### 数据倾斜的处理逻辑

### 列式存储和行式存储的区别，列式存储文件结构是怎么样的

### 并行和并发的区别

### 对JVM的理解

### 堆和栈区别

### 垃圾回收算法，生产中有使用调优过吗

### 创建线程的方式

### flink checkpoint 和savepoint 的区别



### flink 有shuffle吗

有，但是官方定义为partiiton，支持多种模式

- shuffle：随机分区
- rebalance：轮询
- rescale：也是轮询，当下游算子并行度是上游的整数倍时，会有极高的效率
- broadcast：广播，每个TaskManager上缓存一份数据，但是是存在内存中，数据量不能太大

### Slot是一个对象还是一个线程

对象，通常指的就是TaskSlot 类。

### flink 与spark streaming 的区别

**处理模型**：Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批

（Micro-Batch）的模型。 重点！！！

**架构模型**：

Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、

Executor。

Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。

**时间机制**：

时间机制SparkStreaming支持的时间机制有限，只支持处理时间。 Flink

支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时

间。同时也支持 watermark 机制来处理滞后数据。

**容错机制**：

Spark的CheckPoint只能保证数据不丢失，但是无法保证不重复，Flink使

用两阶段提交来处理这个问题。

### 正向代理和反向代理的区别

正向代理 是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。

反向代理正好相反，对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理 的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容 原本就是它自己的一样。

**区别**：正向代理即是客户端代理, 代理客户端, 服务端不知道实际发起请求的客户端.反向代理即是服务端代理, 代理服务端, 客户端不知道实际提供服务的服务端

### https 用了什么加密

> 目前常见的加密算法有：DES、AES、IDEA 等
>
> 目前常见非对称加密算法：RSA，DSA，DH等。

非对称加密+对称加密

1. 某网站拥有用于非对称加密的公钥A、私钥A’。
2. 浏览器向网站服务器请求，服务器把公钥A明文给传输浏览器。
3. 浏览器随机生成一个用于对称加密的密钥X，用公钥A加密后传给服务器。
4. 服务器拿到后用私钥A’解密得到密钥X。
5. 这样双方就都拥有密钥X了，且别人无法知道它。之后双方所有数据都通过密钥X加密解密即可。

### Java 设计模式

### Redis 主从和哨兵模式的区别

### 对云服务的存储计算分离怎么理解

经过 10 年的发展，网络的性能发生了巨大的变化，从之前主流 100Mb 到 10Gb，增长了100倍，而同时期的 HDD 硬盘的性能基本没有太大变化，倒是单盘的容量增大了很多。**大数据的瓶颈逐渐由 IO 变成了 CPU**

而因为存储和计算耦合在一个集群中，带来了一些其它问题：

1. 在不同的应用或者发展时期，需要不同的存储空间和计算能力配比，使得机器的选型会比较复杂和纠结；
2. 当存储空间或计算资源不足时，只能同时对两者进行扩容，导致扩容的经济效率比较低（另一种扩容的资源被浪费了）；
3. 在云计算场景下，不能实现真正的弹性计算，因为计算集群中也有数据，关闭闲置的计算集群会丢失数据。

而存储计算分离恰好能解决这些问题

## 云天励飞

### spark 发生gc 过多 如何定位及解决

同 spark oom解决

### flink 使用中遇到什么问题

flink 部署任务模块 flink 改写connetor

### 星型模型和雪花模型的区别

### 拉链表解决了什么问题，如何加快查询效率

大表，存在update操作，会查看历史状态

**加速：**

1. 在一些查询引擎中，我们对start_date和end_date做索引，这样能提高不少性能。
2. 保留部分历史数据，比如说我们一张表里面存放全量的拉链表数据，然后再对外暴露一张只提供近3个月数据的拉链表。

### Hbase rowkey 怎么设计

主键+hash

### 面向对象和面向函数的区别

举个做饭的例子

面向函数：1.先放水，2.再放米，3.通电。。。

面向对象：创造个机器人，设定好做法的程序，要做饭的时候启动即可

主要想说明的就是 

- **面向函数**就是分析出解决问题所需要的步骤，然后用函数把这些步骤一步一步实现，使用的时候一个一个依次调用就可以了
- **面向对象** 就是 万事万物皆对象，把问题看作对象的属性和行为，以对象为中心思考解决问题

### Hadoop 触发 rebalance条件

1. 通过人工输入命令启动，会在当前节点启动一个进程 rebalance Server（为了避免给namenode带来过大的负担）
2. rebalance server会向NameNode请求一份数据节点报告，在收到报告之后，使用获得的信息，计算出网络拓扑、集群平均存储使用率，然后把各个数据节点分成过载节点、负载节点、存储使用率高于平均水平的节点和低于平均水平的节点四类，再判断是否有节点处于过载和负载状态（也即过载节点列表和负载节点列表中是否有机器），如果是则继续，否则退出。如果判断可继续，则遍历过载节点列表和负载节点列表以生成Rebalance策略
3. Rebalance Server 向Name Node请求每个source节点的部分块分布报告（partial block report），请求的形式类似，默认size是1GB。所谓部分块报告，是指每次要求和返回的的只是加起来能满足size大小的block的信息，而非全部的block信息。
4. namenode随机挑选一些block，使得block的大小加起来等于请求中size的大小（见上一步，默认1GB），然后将被选中的block信息返回给rebalance server。
5. rebalance server 在返回的这些block信息中挑选出每个source上需要移动的block，直到选出的block的大小达到了前面提到过的阈值（见本节2.b中“如果source节点是过载节点……”一段）或者所有的block都被检查过了一遍，然后发往移动任务队列
6. 所有的block被扫描了一遍后，重复步骤3
7. 所有的移动计划已经完成，并且队列中没有任务之后，重复步骤2

### flink 和sparkstreaming 区别

### Hive 处理数据倾斜



## 滴滴

### 星型模型和雪花模型的区别

雪花模型因为没有数据冗余需要关联，而星型不需要关联，星型效率高；

### 拉链表实现步骤

![image-20210412235413965](/Users/sun9/Library/Application Support/typora-user-images/image-20210412235413965.png)

### 增量数据与全量数据合并，如何实现

- 增量与全量 union 在一起，然后根据id去重

- 比如今天的分区是20210412，更改的数据包含了20210412，20210410的分区，那么

  (旧数据里20210412和20210410的分区的所有数据) Full join （今天的数据），然后用if根据是否为null做处理。

### Redis 为什么用SDS

simple data structure结构如下

``` c
struct  sdsher{
//记录buf中已保存字符的长度
//等于SDS所保存的字符串的长度
int  len;
//记录buf数组中未使用字节的数量
int free;
//字节数组，用于保存字符串
char buf[];
};
```

原因：

- 常数复杂度获取字符串长度

- C语言中实现字符串长度计数的复杂度为O（N）。
  而反观SDS中，因为其结构中保存了len这一属性，所以获取SDS长度的复杂度仅为O（1）。

### HBase block encoding，选择编码还是压缩

**压缩：**Hbase支持的压缩有gzip，snappy，lzo

**编码：**Prefix | Diff | Fast_Diff | Prefix_Tree

> 下面能不说就不说

在读写性能上的影响：

- 压缩：因为压缩是在flush 阶段发生的，所以对写性能没有多大影响。对读阶段而言，如果数据在内存中，则不会有解压，如果是在磁盘中，则会有解压，会造成影响。
- 编码：编码也是在flush 阶段发生的，所以对写性能没有多大影响。数据块是以编码形式缓存到blockcache中的，因此同样大小的blockcache可以缓存更多的数据块，这有利于读性能。另一方面，用户从缓存中加载出来数据块之后并不能直接获取KV，而需要先解码，这却不利于读性能。所以很玄学，得实际测。

根据网上的性能压测，发现只启用Prefix_Tree 性能和cpu 负载 最佳，但是具体生产环境还得再做测试。

### 场景：size为一亿的手机号黑名单，拿这个黑名单来对通话记录流来做过滤，怎么做？布隆过滤器如何实现

利用位图，先把所有黑名单刷进位图里，然后每来一条通话记录，就去位图里去命中，命中了就过滤掉，没命中就保留

### flink 流如何进行数据的去重

- 布隆过滤器

- Code: 每个key维护一个布尔值当作状态，来一条去判断是否之前来过，来过就说明重复。

- SQL:  

  - 针对追加流，用 row_number 
  - 针对撤回流，用distinct

  其实底层调用的还是 `RowTimeDeduplicateFunction` ,这里面的实现逻辑就是 把上一行存进状态里，然后跟当前行判断是否重复，重复的话就把之前行和当前行附上操作类型往下发送，不重复就更新状态

  ```java
  if (generateUpdateBefore || generateInsert) {
              if (preRow == null) {
                  // the first row, send INSERT message
                  currentRow.setRowKind(RowKind.INSERT);
                  out.collect(currentRow);
              } else {
                  if (generateUpdateBefore) {
                      final RowKind preRowKind = preRow.getRowKind();
                      preRow.setRowKind(RowKind.UPDATE_BEFORE);
                      out.collect(preRow);
                      preRow.setRowKind(preRowKind);
                  }
                  currentRow.setRowKind(RowKind.UPDATE_AFTER);
                  out.collect(currentRow);
              }
          } else {
              currentRow.setRowKind(RowKind.UPDATE_AFTER);
              out.collect(currentRow);
          }
      }
  ```

  其实 upsert_kafka 也是一样的逻辑，在生成执行计划的时候会加入一个优化器`StreamExecChangelogNormalize` ，这里面就会去调用上面说的去重逻辑。

### 撤回流机制

### Kafka的二阶段提交

俩阶段提交：

1. 请求阶段：协调者向每个参与者发送事务请求，当参与者完成事务后会返回给协调者OK信号，如果都返回的是OK，则进入下一个阶段，否则进行回滚。
2. 提交阶段：协调者向参与者发起事务提交通知，参与者提交事务，并释放资源

存在问题：

1. 单点故障：由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。
2. 同步阻塞：执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

### Hive 行转列，列转行函数

## 百丽

### 保证binlog的顺序一致性

### 窗口不支持撤回流，怎么处理一小时之内的聚合业务

- 利用group by 时间字段 去聚合

- 也可以自定义json- format，把操作类型获取出来，当作append流处理，根据操作类型去自定义处理逻辑

### 窗口触发计算机制

trigger，

在 onELement 方法中去判断 当前时间是否大于 窗口关闭时间，如果是，就计算，不是就去调用注册定时器的方法。注册定时器的方法内部会维护一个时间队列，每次会拿传入时间跟队列头部时间进行比较，拿小的的时间去注册定时器，这样就节省了很多资源

### flink 定时器机制（收件箱）

### flink 某些 数据源 水位线不推进

**withIdleness**() 检测空闲数据源

### flink 异步 io 如何实现

主要用在跟数据库的连接上，比如hologres—connetor 就是用了同步和异步俩中方法去实现 lookup 功能的

### flink 背压机制

流控

### flink SQL转换为执行图 过程，如何拆分算子链

### hive或spark SQL 调优

### SQL 谓词下推机制，parquet 文件格式 谓词下推

简单来说就是在生成逻辑执行计划的时候，会去把过滤条件放到靠近数据源的位置，减少扫描数据量

Parquet中File metadata记录了每一个Row group的Column statistic，包括数值列的max/min，字符串列的枚举值信息，可以通过这些信息过滤掉一些不需要访问的row group。

## 腾讯

### 为什么选用Kudu，Kudu 为什么比Hbase 快

其实主要还是根据我们公司的业务场景来定的，我们对写入性能并没有太高要求，反而对查询性能要求很高，而对比这俩个框架，显然kudu 的查询性能比 Hbase 高。

**原因：**

- Hbase 因为支持用户传入 timestamp 来定义数据的版本，所以必须要结合多个storefile进行查询，而kudu 因为不支持用户传入timestamp，所以它的timestamp 是递增的，所以只需要倒序去检索DeltaFile文件就能拿到最新的数据。
- kudu 因为维护了主键的索引，而且在 DiskRowSet 中维护了一个区间树，每个节点中维护有多个RowSet的最小键和最大键，所以在O(logn)时间内就能找到对应的数据
- 纯列式存储可以支持向量化查询。

https://blog.csdn.net/wangyiyungw/article/details/82701414

https://www.jianshu.com/p/5ffd8730aad8

### Hbase 是怎么 提高存储效率

把所有更新操作当做插入来处理

**优化点：**

- 开启压缩或者编码

### 怎么支持用户上传 自定义UDF

URLClassLoader 获取class，然后flink 注册

### lamada 架构如何理解

**lamada：**

- 优点：保存全部历史数据，方便重计算，灵活
- 缺点：难维护

**kappa：**

- 优点：一套代码，批流统一，方便维护
- 缺点：不够灵活

### Flink使用遇到过哪些坑

任务提交，窗口对撤回流开窗，抽mongo cdc join 任务OOM调优

### Flink metric 底层实现



### MySQL 为什么不支持大数据量

因为

## 拳法

### Kafka 消息回溯原理

其底层原理是通过timeindex文件找到对应的相对偏移量，从而得到offset，然后通过调用KafkaConsumer.seek方法，重设其offset

做法

 ``` bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group groupname --reset-offsets --all-topics --to-datetime 2019-09-15T00:00:00.000``` 

### JVM 执行模式

- 解释模式：只使用解释器（-Xint 强制JVM使用解释模式），执行一行JVM字节码就编译一行为机器码。
  - 特点：启动快，但执行慢，适用于大多数代码只会执行一次的情况
- 编译模式：只使用编译器，先将所有的JVM字节码一次编译为机器码，然后一次性执行所有机器码。
  - 特点：启动慢，执行快，适合代码反复执行的场景
- （默认）混合模式：起始阶段采用解释模式，后续会有一个热点代码监测（HotSpot），对热点代码进行编译
  - 热点代码监测主要是看有没有调用多次的方法或者循环

### controller 是不是单例的

spring 默认就是单例，因为这样快，不会每次使用都要去new一个对象

### 如何保证线程安全

线程安全就是多线程访问同一代码，不会产生不确定的结果。

方法有很多：TreadLocal,加锁。

ThreadLocal与像synchronized这样的锁机制是不同的。首先，它们的应用场景与实现思路就不一样，锁更强调的是如何同步多个线程去正确地共享一个变量，ThreadLocal则是为了解决同一个变量如何不被多个线程共享。从性能开销的角度上来讲，如果锁机制是用时间换空间的话，那么ThreadLocal就是用空间换时间。

Spring 的dao,service 用的就是TreadLocal，ThreadLocal 是线程本地变量，每个线程拥有变量的一个独立副本，所以各个线程之间互不影响，保证了线程安全

### LSM 树

LSM树核心思想的核心就是放弃部分读能力，换取写入的最大化能力。LSM Tree ，这个概念就是结构化合并树的意思，它的核心思路其实非常简单，就是**假定内存足够大**，因此不需要每次有数据更新就必须将数据写入到磁盘中，而可以先将最新的数据驻留在内存中，等到积累到足够多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾(因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起)。

### 行式存储和列式存储

**行式存储**：

1、适合随机的增删改查操作;

2、需要在行中选取所有属性的查询操作;

3、需要频繁插入或更新的操作，其操作与索引和行的大小更为相关。

**列式存储**

1、查询过程中，可针对各列的运算并发执行(SMP)，最后在内存中聚合完整记录集，最大可能降低查询响应时间;

2、可在数据列中高效查找数据，无需维护索引(任何列都能作为索引)，查询过程中能够尽量减少无关IO，避免全表扫描;

3、因为各列独立存储，且数据类型已知，可以针对该列的数据类型、数据量大小等因素动态选择压缩算法，以提高物理存储利用率；如果某一行的某一列没有数据，那在列存储时，就可以不存储该列的值，这将比行式存储更节省空间。

4、拿parquet 来举例，得益于parquet 的File metadata，记录了每一个Row group的Column statistic，包括数值列的max/min，字符串列的枚举值信息，可以通过这些信息过滤掉一些不需要访问的row group。**可以说对谓词下推很友好**。

### 垃圾回收算法

### 红黑树 和  AVL树（平衡二叉搜索树） 区别

二者都是**平衡二叉树**，但是区别如下：

1. 红黑是用非严格的平衡来换取增删节点时候旋转次数的降低，任何不平衡都会在三次旋转之内解决，而AVL是严格平衡树，因此在增加或者删除节点的时候，根据不同情况，旋转的次数比红黑树要多。
2. AVL树提供比红黑树更快的查找，因为avl树 严格平衡
3. AVL树存储每个节点的平衡因子或高度，因此每个节点需要存储一个整数，而红黑树每个节点只需要1位信息。
4. 实际应用中，若搜索的次数远远大于插入和删除，那么选择AVL，如果搜索，插入删除次数几乎差不多，应该选择RB。

### flink的非对齐检查点

https://blog.csdn.net/u013939918/article/details/107372805/?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.base&spm=1001.2101.3001.4242

![image-20210803104853270](/Users/sun9/IdeaProjects/library/picture/非对齐检查点.png)

1. 当算子的所有输入流中的第一个屏障到达算子的输入缓冲区时，立即将这个屏障发往下游（输出缓冲区）
2. 由于第一个屏障没有被阻塞，它的步调会比较快，超过一部分缓冲区中的数据。算子会标记两部分数据：一是屏障首先到达的那条流中被超过的数据（上面chanel中的9），二是其他流中位于当前检查点屏障之前的所有数据（下面channel中的9，7）。

**优点**：在 Barrier 进入输入 Channel 就马上开始快照。这可以从很大程度上加快 Barrier 流经整个 DAG 的速度，从而降低 Checkpoint 整体时长。

**缺点**：由于要持久化缓存数据，State Size 会有比较大的增长，磁盘负载会加重。
随着 State Size 增长，作业恢复时间可能增长，运维管理难度增加。

**与对齐检查点的差异**

1. 快照的触发是在接收到第一个 Barrier 时还是在接收到最后一个 Barrier 时。
2. 是否需要阻塞已经接收到 Barrier 的 Channel 的计算。

### flink的至少一次 检查点是如何工作的

有点类似于非对齐检查点，不会去缓存barrier先到的channel的数据，而是直接处理，**但也不会去对这部分数据（barrier之后的数据）进行标记**，等到barrier都到了之后开始状态的保存

https://blog.csdn.net/weixin_44904816/article/details/102675286

### TCP UDP区别

![image-20210909233402696](/Users/sun9/IdeaProjects/library/picture/image-20210909233402696.png)

核心区别就是 ：

- TCP是传输控制协议，是面向连接的通讯协议（如：打电话）
- UDP是用户数据报协议，是面向无连接的通讯协议（如：发短信）

TCP ：点对点，类似于Kafka的ack =-1

UDP： 广播，类似于Kafka的 ack=1

https://blog.csdn.net/weixin_45372436/article/details/100357832

### RPC 和HTTP的区别

主要用于服务间的远程调用

RPC：自定义数据格式，基于原生TCP通信，速度快，效率高，缺点是客户端和服务端需要统一框架和语言

HTTP：规定了数据传输的格式，缺点是消息封装臃肿，优点是无需关注语言的实现，开发很灵活



### TCP 三次握手四次挥手

握手：

![image-20210830111849700](/Users/sun9/IdeaProjects/library/picture/image-20210830111849700.png)

首先Client端发送连接请求报文，Server段接受连接后回复ACK报文，并为这次连接分配资源。Client端接收到ACK报文后也向Server段发生ACK报文，并分配资源，这样TCP连接就建立了。

挥手：

![image-20210830113620023](/Users/sun9/IdeaProjects/library/picture/image-20210830113620023.png)

【注意】中断连接端可以是Client端，也可以是Server端。

假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说"我Client端没有数据要发给你了"，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，"告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息"。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，"告诉Client端，好了，我这边数据发完了，准备好关闭连接了"。Client端收到FIN报文后，"就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，"就知道可以断开连接了"。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！
————————————————
版权声明：本文为CSDN博主「whuslei」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/whuslei/article/details/6667471

### Hive 调优

- 存储：列式存储，开启压缩

- 执行：行列过滤、map join、小文件优化（har归档，merger，JVM重用）、开启本地模式（小于128m或小于4个map则本地化运行），合理设置map，reduce个数

  

  重点说列式存储，开启了列式存储之后，就可以引用列式存储的查询杀器——**向量化查询**

  > 通常查询是每次只处理一行数据，每次处理都要走过较长的代码路径和元数据解释，从而导致CPU使用率非常低
  >
  > 而在向量化查询执行中，每次处理包含多行记录的一批数据，每一批数据中的每一列都会被存储为一个向量（一个原始数据类型的数组），这就极大地减少了执行过程中的方法调用、反序列化和不必要的if-else操作，大大减少CPU的使用时间。

  就像最近颇具争议的 StarRocks(),它为什么敢叫板市面上所有的olap 数据库，就是得益于它的全面向量化查询引擎和MPP架构

### spark 调优

所有调优无非就是针对**CPU，IO，内存**这三点。

- 集群资源的优化：增加 driver，excutor 的内存，核心数
- 算子的优化：reduceby  、 filter 之后 coalesce 、
- RDD的优化：广播变量、缓存、Kryo序列化、调节本地化等待时间（3s）
- Shuffle的优化：reduce缓冲区大小、reduce 拉取间隔时间、拉取失败重试次数、调节sort shuffle阈值
- JVM调优：堆外内存、连接等待时间（防止因为gc时间过长而报错）

### flink 调优

mongo CDC 抽取mongo表，70w的数据给了一个TM，5G内存，跑不动，看Web UI 有反压。

1. 首先把sink 端换成black hole，发现依然反压，排除掉sink端的问题

2. 设置参数，把算子链打断，观看各个算子任务的运行状况，发现问题出在了一个changelog normalize 的算子上

3. 这个算子的源码之前研究upsert-kafka 时有看过，它的作用其实可以简单理解为给数据去重，转为标准的changelog 流。实现原理是会把同一个key的上一行数据存入状态，然后当前行会去取这个状态，如果状态不存在，就是insert，存在的话，就是上一行为 upsert-before,当前行为 upsert-after。

4. 那么问题的答案就出来了，就是因为它底层回去存储每个key的状态，而我快照阶段所有key都是非重复的，所以就直接把内存占满了

   ```java
   static void processLastRowOnChangelog(
               RowData currentRow,
               boolean generateUpdateBefore,
               ValueState<RowData> state,
               Collector<RowData> out) throws Exception {
           RowData preRow = state.value();
           RowKind currentKind = currentRow.getRowKind();
           if (currentKind == RowKind.INSERT || currentKind == RowKind.UPDATE_AFTER) {
               if (preRow == null) {
                   // the first row, send INSERT message
                   currentRow.setRowKind(RowKind.INSERT);
                   out.collect(currentRow);
               } else {
                   if (generateUpdateBefore) {
                       preRow.setRowKind(RowKind.UPDATE_BEFORE);
                       out.collect(preRow);
                   }
                   currentRow.setRowKind(RowKind.UPDATE_AFTER);
                   out.collect(currentRow);
               }
               // normalize row kind
               currentRow.setRowKind(RowKind.INSERT);
               // save to state
               state.update(currentRow);
           } else {
               // DELETE or UPDATER_BEFORE
               if (preRow != null) {
                   // always set to DELETE because this row has been removed
                   // even the the input is UPDATE_BEFORE, there may no UPDATE_AFTER after it.
                   preRow.setRowKind(RowKind.DELETE);
                   // output the preRow instead of currentRow,
                   // because preRow always contains the full content.
                   // currentRow may only contain key parts (e.g. Kafka tombstone records).
                   out.collect(preRow);
                   // clear state as the row has been removed
                   state.clear();
               }
               // nothing to do if removing a non-existed row
           }
       }
   ```

   

5. 解决方案：

   - 根据flink的内存管理，这一部分主要属于 manage meomory ，所以需要调大这个manage momoery 的占比。快照阶段结束后再调小内存
   - 在社区里跟作者讨论了一下，如果能在数据进来的时候就判断是否是快照阶段，然后快照阶段跳过normalize，这样就能减少状态存储的压力了，但是还没有时间去实现



### OLAP 一般用哪些技术

- MPP架构（大规模并行引擎）：Spark SQL ，Presto，Impala-kudu，
- 预处理架构：Druid，Kylin
- 搜索引擎架构： ES
- 列式数据库： ClickHouse

### Flink窗口 tigger 

```java
 public TriggerResult onElement(
            Object element, long timestamp, TimeWindow window, TriggerContext ctx)
            throws Exception {
        if (window.maxTimestamp() <= ctx.getCurrentWatermark()) {
            // if the watermark is already past the window fire immediately
            return TriggerResult.FIRE;
        } else {
            ctx.registerEventTimeTimer(window.maxTimestamp());
            return TriggerResult.CONTINUE;
        }
    }
```



registerEventTimeTimer 并不是每来一条都回去注册一个定时器，而是内部维护了一个queue，每次会拿当前时间跟队列头部的时间进行比较，选最近的时间去注册

