## 介绍



## PA

### Flink submit on yarn 的API改造

查看flink-yarn 源码，发现flink on yarn 的部署都是先createYarnClusterDescriptor，通过传入flink的配置（运行资源，引擎jar包），yarnClient,yarnConf(加载Hadoop conf,

```
// 设置顶层hdfs实现为DistributedFileSystem
configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
```

),yarn回调函数（可查看集群信息），然后调用deployApplicationCluster（运行配置，applicationName），即可完成部署。

### 计算引擎jar包如何设计

​	基于Flink的设计思想，有Source，Transform，Sink阶段

- Source: 执行映射表的逻辑
- Transform: 利用StatamentSet,将多个执行SQL糅合为一个任务，节约资源
- Sink: 维护了一个catalog集合，根据输出数据源动态加载catalog去执行DDL操作

PS：想利用Spirngboot 创建对象是单例的特性，但是发现1.9版本的flink跟 SpringBoot整合有问题，俩个运行的不在同一个JVM上，会找不到对象，而且也不想引多余的jar包，所以自己实现了一下IOC来自动注入。

### 多个表抽数的时候，如果一个表出错，怎么恢复

只能重跑该任务下的所有表，但是表并不会很多，而且从维护了检查点，从检查点恢复也不会重跑很多数据。

- 任务量：一个任务最多只能包含一个database下10个小表，或者一个database下的5个大表。

### flink 检查点对齐原理

分布式快照，将检查点的保存和数据处理分离开：

1. jobManager 会向source 发送一个带有新checkpoint ID 的信息，以此行为开启检查点
2. source 接收到信息后 会先将状态存入状态后端，然后通知jobmanager完成，接着开始广播checkpoint barrier.
3. 下游算子会等待所有分区都接收到barrier，已经接收到barrier的分区会将接下来的数据缓存起来，而未接收到的barrier的分区则做正常的数据处理，当所有分区都接收到barrier的时候就会触发检查点的保存，通知jobmanager,然后将barrier往下游发送。
4. 当Sink 端也向job manager 确认完毕后，本次checkpoint 完成。

这里得提一下，当任务将其状态保存到检查点时，它其实处于一个阻塞状态，而状态又可能比较大，有可能会阻塞个几分钟，所以我们选用的是支持异步快照的RocksDB，它会创建一个本地的副本，然后开启另一个线程去复制到远程存储，减少了任务的等待时间。

### flink checkpoint 什么情况会失败

先打一套检查点对齐原理----

在这中间的任何环节都可能会失败，但是在生产中最常碰见的就是数据倾斜导致的checkpoint超时

- 扯到

### 状态特别大如何解决（计算一个小时PV）？窗口如何设计？

### flink 源码级别的BUG诊断

当使用Kudu- connector往Kudu中写数时，因为connector内部版本使用的是flink1.11，

### HIVE SQL 的执行过程

### HBASE REGION 的Rebalance 触发条件



