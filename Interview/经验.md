## 项目介绍

### 平台

​	之前公司抽数用的是第三方的软件，但是存在支持数据源少，对数据库负载高的问题，为了解决这个问题，公司决定划分平台组来开发自研的抽数平台。但是我是有点自己的想法的，之前我有看过网易有数那套平台的相关文章，我就想其实我们要做的产品，跟这个是相似的。所以借鉴了目前市面上成熟的平台的一些经验，开发了元数据平台，抽数平台，任务监控平台，任务部署平台，统一计算引擎，支持配置多集群，多数据源，多张表,经过测试，吞吐量能达到每分钟千万级。接下来我简单讲下各个模块的原理：

- 抽数平台：利用debezium 作为一个插件，配合Kafka connect 来将业务库里的数据都发往Kafka中，之后通过Flink SQL 来写到对应的数据源（kudu，upsert_kafka等），值得一提的是，在往kudu里写的时候发现目前开源的connector使用起来有问题，不支持撤回流写入（识别不了主键），时态表等功能，我们自己改了源码，提供了这些功能。
- 元数据平台：在用户选择source表时，会从对应数据源拉取对应的元数据到我们平台的元数据库，然后根据用户选择的输出源进行自动的字段映射，同时也维护一份输出表元数据，而维护的这些元数据通过自动拼接DDL来在抽数平台和SQL开发平台上实现无DDL编程。
- 任务监控平台：基于zookeeper的监听器机制，每次任务启动时去注册一个临时节点，并启动监听器，一旦任务失败则会根据报警策略报警。
- 任务部署：目前只支持flink on yarn 部署，后续打算将spark，k8s集成进去。用户在前端传入相应的运行配置，抽数平台只需要点点点，sql开发平台需要输入transform SQL，然后把这些配置序列化存到redis配置中心中。其部署任务那块，主要参考flink-yarn 模块，先通过传入flink的配置（运行资源，引擎jar包），yarnClient,yarn conf ，yarn回调函数，生成集群描述器，然后调用部署方法即可完成部署
- 统一计算引擎：有点类似于zeppelin，分source，transform，sink，**目的是使用同一套代码，最大限度的去兼容不同的计算引擎，集群环境**。基于SQL开发环境，flink ，spark各包含一个main方法，主要工作就是读取配置中心，封装成方法入参，底层还是使用的env.excutesql（），不过为了避免一个表生成一个job，采用了StatmentSet,把任务都封装为了一个job，最大化节约资源。

### 离线数仓

之前公司数仓存在的问题主要有：

1. SQL冗余：每个宽表都是从ods直接取的，没有中间的建模过程，导致一个SQL有700多行是常有的事，很难维护。
2. 小文件：使用Data pipeline实时同步数据时，会产生大量小文件
3. 建表DDL与业务逻辑SQL混在一起，不易维护

所以参照领域驱动模型的一些思想，有以下几层结构：

1. driver：方法入口
2. Service: 具体的转化逻辑
3. repostity:通用方法，比如读取数据，写入数据
4. DDL：主要存放表的DDL，并且会维护一个JSON文件，存了表名和DDL文件的映射，到时候会根据表名去获取DDL语句来进行创建表的操作

同时我们也写了一个合并小文件的脚本，原理就是读出来写入到临时表，然后写回到表里,控制reduce个数为2；（支持动态传入）

总体建模有四层：source（主要针对实时同步的表，会含有DDL标识），ods，dwd，dim，DM

总体的数据流向：

- 业务库：polarDB，PG
- 抽数中间件：sqoop，dataWorks,datapipline
- 数仓：Hive
- 宽表交互：ADBPG

**离线数仓重构的效果有什么量化指标来衡量**？

## 平安

### Flink submit on yarn 的API改造

查看flink-yarn 源码，发现flink on yarn 的部署都是先createYarnClusterDescriptor，通过传入flink的配置（运行资源，引擎jar包），yarnClient,yarnConf(加载Hadoop conf,

```
// 设置顶层hdfs实现为DistributedFileSystem
configuration.set("fs.hdfs.impl", "org.apache.hadoop.hdfs.DistributedFileSystem");
```

),yarn回调函数（可查看集群信息），然后调用deployApplicationCluster（运行配置（内存，核心数），applicationName），即可完成部署。

### 计算引擎jar包如何设计

​	基于Flink的设计思想，有Source，Transform，Sink阶段

- Source: 执行映射表的逻辑
- Transform: 利用StatamentSet,将多个执行SQL糅合为一个任务，节约资源
- Sink: 维护了一个catalog集合，根据输出数据源动态加载catalog去执行DDL操作

PS：想利用Spirngboot 创建对象是单例的特性，但是发现1.9版本的flink跟 SpringBoot整合有问题，俩个运行的不在同一个JVM上，会找不到对象，而且也不想引多余的jar包，所以自己实现了一下IOC来自动注入。

### 多个表抽数的时候，如果一个表出错，怎么恢复

只能重跑该任务下的所有表，但是表并不会很多，而且从维护了检查点，从检查点恢复也不会重跑很多数据。

- 任务量：一个任务最多只能包含一个database下10个小表，或者一个database下的5个大表。

### flink 检查点对齐原理

分布式快照，将检查点的保存和数据处理分离开：

1. jobManager 会向source 发送一个带有新checkpoint ID 的信息，以此行为开启检查点
2. source 接收到信息后 会先将状态存入状态后端，然后通知jobmanager完成，接着开始广播checkpoint barrier.
3. 下游算子会等待所有分区都接收到barrier，已经接收到barrier的分区会将接下来的数据缓存起来，而未接收到的barrier的分区则做正常的数据处理，当所有分区都接收到barrier的时候就会触发检查点的保存，通知jobmanager,然后将barrier往下游发送。
4. 当Sink 端也向job manager 确认完毕后，本次checkpoint 完成。

这里得提一下，当任务将其状态保存到检查点时，它其实处于一个阻塞状态，而状态又可能比较大，有可能会阻塞个几分钟，所以我们选用的是支持异步快照的RocksDB，它会创建一个本地的副本，然后开启另一个线程去复制到远程存储，减少了任务的等待时间。

### flink的非对齐检查点

https://blog.csdn.net/nazeniwaresakini/article/details/107954076

1. 当算子的所有输入流中的第一个屏障到达算子的输入缓冲区时，立即将这个屏障发往下游（输出缓冲区）
2. 由于第一个屏障没有被阻塞，它的步调会比较快，超过一部分缓冲区中的数据。算子会标记两部分数据：一是屏障首先到达的那条流中被超过的数据，二是其他流中位于当前检查点屏障之前的所有数据（当然也包括进入了输入缓冲区的数据）。
3. 



### flink checkpoint 什么情况会失败

先打一套检查点对齐原理----

在这中间的任何环节都可能会失败，但是在生产中最常碰见的就是数据倾斜导致的checkpoint超时

- 扯到生产上碰到的计算一个小时内每个地区的PV，最开始采用的方法是直接开窗再sum，然后发现检查点失败，某几个task还出现了反压，后来的解决思路：按照（地区+随机数）keyby之后开窗，然后用聚合算子aggregate，然后再keyby（去除随机数），聚合，算出最终值

### 状态特别大如何解决（计算一个小时PV）？窗口如何设计？

- 状态大可以开启异步快照（默认都是开启的），
- 使用ROCKsDB的话还可以开启增量快照，
- 同时也可以增大检查点之间的间隔，避免浪费过多的资源在检查点保存上

### flink 源码级别的BUG诊断

当使用Kudu- connector往Kudu中写数时，发现不支持撤回流写入，跟源码发现他内部走的是老的API,输出用的是DataStreamSink，在SQL中指定的主键不生效，后来自己定义了Kudu的动态数据源DynamicTableSink，使用的输出是SinkFunctionProvider，并且在工厂生产之前传入了shchema信息，以便后续做key的校验（参考JDBCSINk）打通了kafka到kudu的撤回流写入功能。

### HIVE SQL 的执行过程

Hive和spark 的Sql解析器是使用的Antlr4，但是优化过程也借用了Calcite的基于代价的优化策略，flink用的是Calcite。

- 基于代价的优化策略，会单独起一个任务，代价衡量值是cpu和IO，计算各个Opertator Tree节点的代价总和，从而得到代价最小的执行计划

根据Hive 的架构图说，

![preview](https://segmentfault.com/img/remote/1460000038359370/view)

![preview](https://segmentfault.com/img/remote/1460000038359371/view)

1. Parser：将sql解析为AST（抽象语法树），会进行语法校验，AST本质还是字符串。

2. Analyzer：语法解析，生成QB（query block）
3. 逻辑执行计划解析，生成一堆Opertator Tree
4. 进行逻辑执行计划优化，生成一堆优化后的Opertator Tree
5. 物理执行计划解析，生成tasktree
6. 进行物理执行计划优化，生成优化后的tasktree，该任务即是集群上的执行的作业

### HBASE REGION 的Rebalance 触发条件

- 触发条件：
  - 自动触发：如果开启balance_switch 参数，在HMaster中，后台会起一个线程定期检查是否需要进行rebalance，线程叫做BalancerChore。线程每隔 hbase.balancer.period会定期执行 master.balance()函数，配置项默认300000毫秒，5分钟。每次balance最多执行hbase.balancer.max.balancing，如果没有配置，则使用hbase.balancer.period配置项的值。
  - 手动触发：balancer
- 负载均衡原理：

1. 计算均衡值的区间范围，通过总Region个数以及RegionServer节点个数，算出平均Region个数，然后在此基础上计算最小值和最大值

   ```
   # hbase.regions.slop 权重值，默认为0.2
   最小值 = Math.floor(平均值 * (1-0.2))
   最大值 = Math.ceil(平均值 * (1+0.2))
   ```

2. 遍历超过Region最大值的RegionServer节点，将该节点上的Region值迁移出去，直到该节点的Region个数小于等于最大值的Region

3. 遍历低于Region最小值的RegionServer节点，分配集群中的Region到这些RegionServer上，直到大于等于最小值的Region

4. 负责上述操作，直到集群中所有的RegionServer上的Region个数在最小值与最大值之间，集群才算到达负载均衡，之后，即使再次手动执行均衡命令，HBase底层逻辑判断会执行忽略操作

## 极光

### Kafka如何保证数据的稳定性

- ISR机制
- ack应答机制
- 故障处理：HW、LEO

### Kafka partition 消费堵塞原因分析（感觉像问的是消费能力不足）

1）如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）

2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间<生产速度），使处理的数据小于生产的数据，也会造成数据积压。

### Kafka partition 和消费者数量关系

partition 数量 >= consumer 数量

再讲一下消费策略：

range - 首先会统计一个消费者，一共订阅了哪些主题！以主题为单位，  根据     主题的分区数 / 当前主题订阅的消费者个数 ，根据结果，进行范围的分配！

Robin - 以主题为单位，将主题的分区进行排序，排序后采取轮询的策略，将主题轮流分配到订阅这个主题的消费者上！

### 数据去重方式，以及优缺点

- distinct ：对select 后面所有字段进行去重。会把所有数据发到一个reduce里面，效率较低
- Group  by: 对group by 后的字段进行去重，会有多少组发送给多少个reduce，效率高
- Row_number:语法上比group by更加灵活，但是有个排序的过程，效率稍低。

### map ,map partition 的区别

- map是对rdd中的每一个元素进行操作；一个function要执行的次数等于元素的个数

- mapPartitions则是对rdd中的每个分区的迭代器进行操作，把一个分区的数据都收集起来进行消费，一个分区只执行一次function，但是数据量大的时候就会OOM。

### spark OOM 怎么分析以及解决方法

​	首先需要了解Spark 的内存管理机制，他主要分为三块，缓存内存，计算内存，其他内存（存数据结构，元数据等信息），缓存内存和计算内存之间是动态管理的。

​	接下来在说一下OOM发生的地点，Driver端和Excutor 端。

Driver ： 1. 定义了一个很大的变量（缓存）

​				2.把所有数据拉到driver 处理（缓存）

​				3.spark UI 也会占据一部分内存（计算）

解决方案：加内存

Excutor:

- 1.广播了一个大变量到blockmanager（缓存）

- 2.数据倾斜（计算）

- 3.reduce缓冲区过大（计算）

解决方案：加内存-》针对数据倾斜：1.提高shuffle并行度2.先过滤

3.加盐4.join 类（mapjoin ， 稀释扩容）

### Hbase rowKey设计

原则：长度（不要过长，会影响检索效率），散列，唯一

id+create_time 反转

### Flink 保证精准一次性

- Source:可重设数据读取位置

- Transform:检查点一致性算法

- Sink：

  - 事务性写入：

    - 俩阶段提交：

      1. 请求阶段：协调者向每个参与者发送事务请求，当参与者完成事务后会返回给协调者OK信号，如果都返回的是OK，则进入下一个阶段，否则进行回滚。
      2. 提交阶段：协调者向参与者发起事务提交通知，参与者提交事务，并释放资源

      存在问题：

      1. 单点故障：由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。
      2. 同步阻塞：执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。

  - 幂等性写入

### HashMap 底层原理

数组+链表+红黑树

链表节点超过8 -》红黑树

红黑树节点小于6 -〉 链表

扩容： 底层会创建一个长度为16的数组(Node类型)，加载因子为0.75，当我们向集合中添加的数据超过12 （16 * 0.75）时便会扩容，扩容为原来的2倍。

### 进程和线程的区别

进程是操作系统资源分配的基本单位，而线程是CPU调度和分派的基本单位

进程有自己独立的地址空间，每启动一个进程，系统都会为其分配地址空间，建立数据表来维护代码段、堆栈段和数据段，线程没有独立的地址空间，它使用相同的地址空间共享数据；

内存方面有点类似于容器与task的区别，进程有一个共享工作内存，线程可以使用这个内存，但是线程与线程之间不能互相使用它们的工作内存。

### kudu 和Hbase的异同

- 不同：

1. Hbase依托于zookeeper，利用其来存储元数据表位置，master位置，RegionServer的工作状态，而Kudu将这些工作都交给了Master
2. Hbase将数据持久化这部分的功能交给了Hadoop中的HDFS，最终组织的数据存储在HDFS上。Kudu自己将存储模块集成在自己的结构中，内部的数据存储模块通过Raft协议来保证leader Tablet和replica Tablet内数据的强一致性，和数据的高可靠性。为什么不像HBase一样，利用HDFS来实现数据存储，猜测可能是因为HDFS读小文件时的时延太大，所以Kudu自己重新完成了底层的数据存储模块，并将其集成在TServer中。
3. hbase列族存储，kudu列式存储
4. 针对相同主键更新操作的数据，Hbase是允许多版本的数据存在的，所以直接当做一条新插入的数据，而kudu只允许有一条存在，所以会将更新操作单独存放于内存，等待刷新或读取的时候再合并。

- 相同：

1. 都是通过数据的timestamp字段来实现mvcc，但是**hbase可以手动指定，kudu不能**

### Kudu 的调参优化

### Hbase为什么采用列族存储

因为Hbase底层存储依托于HDFS，如果行式存储不适用于做loap查询，而列式的话，因为一列一个文件，可能会有小文件的问题，所以采用了列族存储（个人观点）

### 为什么Kudu 流式写入性能为什么能这么高，里面有批处理的逻辑吗

主要得益于LSM树存储引擎，先顺序写到wal文件中

### Spark 和MR的区别

1. **基于内存数据处理模式**

   spark是基于内存进行数据处理的，MapReduce是基于磁盘进行数据处理

   的

2. **DAG计算模型**

   Spark 计算比 MapReduce 快的根本原因在于 DAG 计算模型。一般而言，

   DAG 相比MapReduce 在大多数情况下可以减少 shufflfflffle 次数。Spark 的 

   DAGScheduler 相当于一个改进版的 MapReduce，如果计算不涉及与其

   他节点进行数据交换，Spark 可以在内存中一次性完成这些操作，也就是

   中间结果无须落盘，减少了磁盘 IO 的操作。但是，如果计算过程中涉及数

   据交换，Spark 也是会把 shufflfflffle 的数据写磁盘的！

3. **资源申请粒度**

   spark是粗粒度资源申请，也就是当提交spark application的时候，

   application会将所有的资源申请完毕，如果申请不到资源就等待，如果申

   请到资源才执行application，task在执行的时候就不需要自己去申请资

   源，task执行快，当最后一个task执行完之后task才会被释放。

   **优点是执行速度快，缺点是不能使集群得到充分的利用**

   MapReduce是细粒度资源申请，当提交application的时候，Task执行时，

   自己申请资源，自己释放资源，task执行完毕之后，资源立即会被释放，

   task执行的慢，application执行的相对比较慢。

   优点是集群资源得到充分利用，缺点是application执行的相对比较慢。

   **Spark是基于内存的，而MapReduce是基于磁盘的迭代**

### 说一说Shuffle 的理解

### 数据倾斜的处理逻辑

### 列式存储和行式存储的区别，列式存储文件结构是怎么样的

### 并行和并发的区别

### 对JVM的理解

### 堆和栈区别

### 垃圾回收算法，生产中有使用调优过吗

### 创建线程的方式

### flink checkpoint 和savepoint 的区别

### flink 有shuffle吗

有，但是官方定义为re

### Slot是一个对象还是一个线程

### flink 与spark streaming 的区别

**处理模型**：Flink 是标准的实时处理引擎，基于事件驱动。而 Spark Streaming 是微批

（Micro-Batch）的模型。 重点！！！

**架构模型**：

Spark Streaming 在运行时的主要角色包括：Master、Worker、Driver、

Executor。

Flink 在运行时主要包含：Jobmanager、Taskmanager和Slot。

**时间机制**：

时间机制SparkStreaming支持的时间机制有限，只支持处理时间。 Flink

支持了流处理程序在时间上的三个定义：处理时间、事件时间、注入时

间。同时也支持 watermark 机制来处理滞后数据。

**容错机制**：

Spark的CheckPoint只能保证数据不丢失，但是无法保证不重复，Flink使

用两阶段提交来处理这个问题。

### 正向代理和反向代理的区别

正向代理 是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。客户端必须要进行一些特别的设置才能使用正向代理。

反向代理正好相反，对于客户端而言它就像是原始服务器，并且客户端不需要进行任何特别的设置。客户端向反向代理 的命名空间(name-space)中的内容发送普通请求，接着反向代理将判断向何处(原始服务器)转交请求，并将获得的内容返回给客户端，就像这些内容 原本就是它自己的一样。

**区别**：正向代理即是客户端代理, 代理客户端, 服务端不知道实际发起请求的客户端.反向代理即是服务端代理, 代理服务端, 客户端不知道实际提供服务的服务端

### https 用了什么加密

### Java 设计模式

### Redis 主从和哨兵模式的区别

### 对云服务的存储计算分离怎么理解

经过 10 年的发展，网络的性能发生了巨大的变化，从之前主流 100Mb 到 10Gb，增长了100倍，而同时期的 HDD 硬盘的性能基本没有太大变化，倒是单盘的容量增大了很多。**大数据的瓶颈逐渐由 IO 变成了 CPU**

而因为存储和计算耦合在一个集群中，带来了一些其它问题：

1. 在不同的应用或者发展时期，需要不同的存储空间和计算能力配比，使得机器的选型会比较复杂和纠结；
2. 当存储空间或计算资源不足时，只能同时对两者进行扩容，导致扩容的经济效率比较低（另一种扩容的资源被浪费了）；
3. 在云计算场景下，不能实现真正的弹性计算，因为计算集群中也有数据，关闭闲置的计算集群会丢失数据。

而存储计算分离恰好能解决这些问题

## 滴滴

### 星型模型和雪花模型的区别

雪花模型因为没有数据冗余需要关联，而星型不需要关联，星型效率高；

### 拉链表实现步骤

![image-20210412235413965](/Users/sun9/Library/Application Support/typora-user-images/image-20210412235413965.png)

### 增量数据与全量数据合并，如何实现

- 增量与全量 union 在一起，然后根据id去重

- 比如今天的分区是20210412，更改的数据包含了20210412，20210410的分区，那么

  (旧数据里20210412和20210410的分区的所有数据) Full join （今天的数据），然后用if根据是否为null做处理。

### Redis 为什么用SDS

simple data structure结构如下

``` c
struct  sdsher{
//记录buf中已保存字符的长度
//等于SDS所保存的字符串的长度
int  len;
//记录buf数组中未使用字节的数量
int free;
//字节数组，用于保存字符串
char buf[];
};
```

原因：

- 常数复杂度获取字符串长度

- C语言中实现字符串长度计数的复杂度为O（N）。
  而反观SDS中，因为其结构中保存了len这一属性，所以获取SDS长度的复杂度仅为O（1）。

### HBase block encoding

通过某种算法，对Data Block中的数据进行压缩，这样Block的Size小了，放到Block Cache中数据的就多了，可以增加查询效率。

### 场景：size为一亿的手机号黑名单，拿这个黑名单来对通话记录流来做过滤，怎么做？布隆过滤器如何实现

利用位图，先把所有黑名单刷进位图里，然后每来一条通话记录，就去位图里去命中，命中了就过滤掉，没命中就保留

### flink 流如何进行数据的去重

### 撤回流机制

### Kafka的二阶段提交

### Hive 行转列，列转行函数





## 拳法

### Kafka 消息回溯原理

其底层原理是通过timeindex文件找到对应的相对偏移量，从而得到offset，然后通过调用KafkaConsumer.seek方法，重设其offset

做法

 ``` bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group groupname --reset-offsets --all-topics --to-datetime 2019-09-15T00:00:00.000``` 

### JVM 执行模式

- 解释模式：只使用解释器（-Xint 强制JVM使用解释模式），执行一行JVM字节码就编译一行为机器码。
  - 特点：启动快，但执行慢，适用于大多数代码只会执行一次的情况
- 编译模式：只使用编译器，先将所有的JVM字节码一次编译为机器码，然后一次性执行所有机器码。
  - 特点：启动慢，执行快，适合代码反复执行的场景
- （默认）混合模式：起始阶段采用解释模式，后续会有一个热点代码监测（HotSpot），对热点代码进行编译
  - 热点代码监测主要是看有没有调用多次的方法或者循环

### controller 是不是单例的

spring 默认就是单例，因为这样快，不会每次使用都要去new一个对象

### 如何保证线程安全

线程安全就是多线程访问同一代码，不会产生不确定的结果。

方法有很多：TreadLocal,加锁。

ThreadLocal与像synchronized这样的锁机制是不同的。首先，它们的应用场景与实现思路就不一样，锁更强调的是如何同步多个线程去正确地共享一个变量，ThreadLocal则是为了解决同一个变量如何不被多个线程共享。从性能开销的角度上来讲，如果锁机制是用时间换空间的话，那么ThreadLocal就是用空间换时间。

Spring 的dao,service 用的就是TreadLocal，ThreadLocal 是线程本地变量，每个线程拥有变量的一个独立副本，所以各个线程之间互不影响，保证了线程安全

### LSM 树

LSM树核心思想的核心就是放弃部分读能力，换取写入的最大化能力。LSM Tree ，这个概念就是结构化合并树的意思，它的核心思路其实非常简单，就是**假定内存足够大**，因此不需要每次有数据更新就必须将数据写入到磁盘中，而可以先将最新的数据驻留在内存中，等到积累到足够多之后，再使用归并排序的方式将内存内的数据合并追加到磁盘队尾(因为所有待排序的树都是有序的，可以通过合并排序的方式快速合并到一起)。

### 行式存储和列式存储

**行式存储**：

1、适合随机的增删改查操作;

2、需要在行中选取所有属性的查询操作;

3、需要频繁插入或更新的操作，其操作与索引和行的大小更为相关。

**列式存储**

1、查询过程中，可针对各列的运算并发执行(SMP)，最后在内存中聚合完整记录集，最大可能降低查询响应时间;

2、可在数据列中高效查找数据，无需维护索引(任何列都能作为索引)，查询过程中能够尽量减少无关IO，避免全表扫描;

3、因为各列独立存储，且数据类型已知，可以针对该列的数据类型、数据量大小等因素动态选择压缩算法，以提高物理存储利用率；如果某一行的某一列没有数据，那在列存储时，就可以不存储该列的值，这将比行式存储更节省空间。

### 垃圾回收算法

### 红黑树 和AVL树 区别

二者都是**平衡二叉树**，但是区别如下：

1. 红黑是用非严格的平衡来换取增删节点时候旋转次数的降低，任何不平衡都会在三次旋转之内解决，而AVL是严格平衡树，因此在增加或者删除节点的时候，根据不同情况，旋转的次数比红黑树要多。
2. AVL树提供比红黑树更快的查找，因为avl树 严格平衡
3. AVL树存储每个节点的平衡因子或高度，因此每个节点需要存储一个整数，而红黑树每个节点只需要1位信息。
4. 实际应用中，若搜索的次数远远大于插入和删除，那么选择AVL，如果搜索，插入删除次数几乎差不多，应该选择RB。



